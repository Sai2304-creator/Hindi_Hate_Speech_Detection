{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/victorknox/Hate-Speech-Detection-in-Hindi/blob/main/Hate_Speech_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeHmXPtnwN8g"
   },
   "source": [
    "# Subjective Data Analysis\n",
    "- Used Sentiment lexicon resource for hindi called Hindi Sentiwordnet.\n",
    "- It has around 3000 prior-polarity subjective clues with POS tag, positive score, negative score and related terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "996HeBAnhq3k",
    "outputId": "6f72748a-9d73-48e3-d460-316aeaa73a35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['अनौपचारिक']\n",
      "['मृत']\n",
      "['परवर्ती']\n",
      "['अच्छा', 'बढ़िया']\n",
      "['सौभाग्यशाली', 'खुशकिस्मत', 'खुशनसीब', 'तक़दीर_वाला', 'नसीब_वाला', 'भाग्यवान', 'भाग्यशाली', 'ख़ुशक़िस्मत', 'ख़ुशनसीब']\n"
     ]
    }
   ],
   "source": [
    "SUBJCLUE = []                     \n",
    "\n",
    "with open('SUBJCLUE.txt', encoding=\"utf8\") as f:   \n",
    "    for line in f:               \n",
    "      x = line.split()            \n",
    "      x[4] = x[4].split(',')      \n",
    "      SUBJCLUE.append(x)          \n",
    "\n",
    "\n",
    "# printing the first 5 rows\n",
    "for key in SUBJCLUE[:5]:\n",
    "  print(key[4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AADg4qY8qoyZ"
   },
   "source": [
    "## Reading the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmgETknXXHph"
   },
   "source": [
    "Note: The Dataset should be a csv file with Fields corresponding to Unique ID, Post, Labels Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gV-RT5LurrDr",
    "outputId": "423f29d0-a889-421a-b688-20a43b89a0ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of rows: 2259\n",
      "Field names are:Unique ID, Post, Labels Set\n",
      "['1', 'दृढ़ इच्छा शक्ति से परिपूर्ण प्रणबदा के लिए देशहित सर्वोच्च रहा।\\n\\nउनका निधन हम सब के लिए अपूरणीय क्षति है।\\nईश्वर दिवंगत आत्मा को अपने श्रीचरणों में स्थान दें। शोक संतप्त परिजनों के प्रति संवेदनाएं।\\nऊं शांति!!!', 'non-hostile', 0]\n",
      "['2', 'भारतीय जनता पार्टी rss वाले इतने गिरे हुए हैं जहां मैं रहती हूं वहां मेरी जासूसी  करा रहें है उसकी जासूस की पहचान मुझे अच्छी तरह है rss बीजेपी वाले की जासूस दिल्ली में कौन है उत्तर प्रदेश में कौन है हरियाणा राजस्थान में कौन है सबकी पहचान है मुझे मेरी नजर से बच नहीं सकते हो', 'defamation', 0]\n",
      "['3', 'कोरोना से निपटने की तैयारी / दिल्ली में 10 हजार बेड वाला दुनिया का सबसे बड़ा कोविड केयर सेंटर शुरू, राजनाथ-शाह ने डीआरडीओ के 1 हजार बेड वाले सेंटर का भी उद्घाटन किया\\nhttps://t.co/9rlQowAsFh #Delhi @ArvindKejriwal  @rajnathsingh @AmitShah @DRDO_India @WHO @crpfindia @ITBP_official', 'non-hostile', 0]\n",
      "['4', 'गवर्नर कॉन्फ्रेंस में PM मोदी बोले- शिक्षा नीति में सरकार का दखल कम होना चाहिए\\nhttps://t.co/ZvKgxk6dbd', 'non-hostile', 0]\n",
      "['5', 'यूपी: गाजीपुर में Toilet घोटाला, प्रधान व सचिव ने किया लाखों का गबन, मुर्दों के नाम पर बनवा डाले शौचालय\\n\\n#UP\\nhttps://t.co/hxM1uNNmX2', 'non-hostile', 0]\n"
     ]
    }
   ],
   "source": [
    "import csv                                 \n",
    "\n",
    "\n",
    "filename = \"Dataset/valid.csv\"                      \n",
    "  \n",
    "\n",
    "fields = []                                \n",
    "rows = []                                  \n",
    "  \n",
    "with open(filename, 'r', encoding=\"utf8\") as csvfile:        \n",
    "    csvreader = csv.reader(csvfile)\n",
    "\n",
    "    fields = next(csvreader)\n",
    "  \n",
    "    for row in csvreader:\n",
    "        rows.append(row)\n",
    "  \n",
    "    # get total number of rows\n",
    "    print(\"Total no. of rows: %d\"%(csvreader.line_num))\n",
    "  \n",
    "\n",
    "print('Field names are:' + ', '.join(field for field in fields))\n",
    "  \n",
    "tot = 0\n",
    "for row in rows:\n",
    "    row.append(tot)\n",
    "    # print(row)\n",
    "\n",
    "for row in rows[:5]:\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUHPthTzs42T"
   },
   "source": [
    "## Checking score\n",
    "\n",
    "Finding positive, negative and total scores for each sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dEGBezazs61_",
    "outputId": "4c0ce45a-d393-4df0-edc7-8b1840422748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10530\n"
     ]
    }
   ],
   "source": [
    "count = 0                             \n",
    "for key in SUBJCLUE:                  \n",
    "  subjlist = key[4]                            \n",
    "\n",
    "  for row in rows:                    \n",
    "    if any([subjword in row[1] for subjword in subjlist]):  \n",
    "      count += 1           \n",
    "      pos = float(key[2])  \n",
    "      neg = float(key[3])   \n",
    "      tot = pos - neg       \n",
    "      row[3] += tot\n",
    "\n",
    "# printing the number of occurences of sentiment words in dataset\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imewqNVTN0cp"
   },
   "source": [
    "# Hate Lexicon Growing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7c6WmGyF7oY",
    "outputId": "2a085be0-5499-4361-eca5-c6bdd92ccb47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.4.2-py3-none-any.whl (691 kB)\n",
      "Requirement already satisfied: numpy in c:\\anaconda\\lib\\site-packages (from stanza) (1.20.3)\n",
      "Collecting torch>=1.3.0\n",
      "  Downloading torch-1.13.0-cp39-cp39-win_amd64.whl (167.2 MB)\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from stanza) (4.62.3)\n",
      "Requirement already satisfied: six in c:\\users\\umika kunta\\appdata\\roaming\\python\\python39\\site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (from stanza) (2.26.0)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-4.21.10-cp39-cp39-win_amd64.whl (525 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\anaconda\\lib\\site-packages (from torch>=1.3.0->stanza) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests->stanza) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests->stanza) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\anaconda\\lib\\site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests->stanza) (3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\umika kunta\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234925 sha256=9960d8722dae8a4bef6f46c7f30fede38439bc5f5918bf6bb899aaee1d304448\n",
      "  Stored in directory: c:\\users\\umika kunta\\appdata\\local\\pip\\cache\\wheels\\9a\\b8\\0f\\f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n",
      "Successfully built emoji\n",
      "Installing collected packages: torch, protobuf, emoji, stanza\n",
      "Successfully installed emoji-2.2.0 protobuf-4.21.10 stanza-1.4.2 torch-1.13.0\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (58.0.4)\n",
      "Collecting subzero\n",
      "  Downloading subzero-0.2.4-py3-none-any.whl (14 kB)\n",
      "Collecting PyInstaller\n",
      "  Downloading pyinstaller-5.6.2-py3-none-win_amd64.whl (1.2 MB)\n",
      "Collecting deepmerge\n",
      "  Downloading deepmerge-1.1.0-py3-none-any.whl (8.5 kB)\n",
      "Collecting pyspin\n",
      "  Downloading pyspin-1.1.1-py2.py3-none-any.whl (5.3 kB)\n",
      "Collecting PyRTF3>=0.47.3\n",
      "  Downloading PyRTF3-0.47.5-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\umika kunta\\appdata\\roaming\\python\\python39\\site-packages (from subzero) (21.3)\n",
      "Collecting pywix>=0.2\n",
      "  Downloading pywix-0.2.1.tar.gz (21 kB)\n",
      "Collecting pipdeptree\n",
      "  Downloading pipdeptree-2.3.3-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: PyParsing in c:\\users\\umika kunta\\appdata\\roaming\\python\\python39\\site-packages (from PyRTF3>=0.47.3->subzero) (3.0.9)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (from pywix>=0.2->subzero) (58.0.4)\n",
      "Requirement already satisfied: pip>=8.1.0 in c:\\anaconda\\lib\\site-packages (from pywix>=0.2->subzero) (21.2.4)\n",
      "Requirement already satisfied: wheel>=0.25.0 in c:\\anaconda\\lib\\site-packages (from pywix>=0.2->subzero) (0.37.0)\n",
      "Requirement already satisfied: pywin32-ctypes>=0.2.0 in c:\\anaconda\\lib\\site-packages (from PyInstaller->subzero) (0.2.0)\n",
      "Collecting pefile>=2022.5.30\n",
      "  Downloading pefile-2022.5.30.tar.gz (72 kB)\n",
      "Collecting pyinstaller-hooks-contrib>=2021.4\n",
      "  Downloading pyinstaller_hooks_contrib-2022.13-py2.py3-none-any.whl (250 kB)\n",
      "Collecting altgraph\n",
      "  Downloading altgraph-0.17.3-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: future in c:\\anaconda\\lib\\site-packages (from pefile>=2022.5.30->PyInstaller->subzero) (0.18.2)\n",
      "Collecting futures\n",
      "  Downloading futures-3.0.5.tar.gz (25 kB)\n",
      "  Downloading futures-3.0.4.tar.gz (25 kB)\n",
      "  Downloading futures-3.0.3.tar.gz (24 kB)\n",
      "  Downloading futures-3.0.2.tar.gz (24 kB)\n",
      "  Downloading futures-3.0.1.tar.gz (24 kB)\n",
      "  Downloading futures-3.0.0.tar.gz (24 kB)\n",
      "  Downloading futures-2.2.0-py2.py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: pywix, pefile\n",
      "  Building wheel for pywix (setup.py): started\n",
      "  Building wheel for pywix (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pywix\n",
      "  Building wheel for pefile (setup.py): started\n",
      "  Building wheel for pefile (setup.py): finished with status 'done'\n",
      "  Created wheel for pefile: filename=pefile-2022.5.30-py3-none-any.whl size=69374 sha256=8b59164758d47a04f737241eccae635bf39b5f01d0afc0386433d8e7a9fb7d45\n",
      "  Stored in directory: c:\\users\\umika kunta\\appdata\\local\\pip\\cache\\wheels\\f1\\fe\\01\\5536332fc49e010786c52886ac14f72073dd65fed3c3d8945c\n",
      "Successfully built pefile\n",
      "Failed to build pywix\n",
      "Installing collected packages: pyinstaller-hooks-contrib, pefile, futures, altgraph, pywix, pyspin, PyRTF3, PyInstaller, pipdeptree, deepmerge, subzero\n",
      "    Running setup.py install for pywix: started\n",
      "    Running setup.py install for pywix: still running...\n",
      "    Running setup.py install for pywix: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\anaconda\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\futures_a24b25ba35674d258b6cc8ad478fcdd6\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\futures_a24b25ba35674d258b6cc8ad478fcdd6\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-pip-egg-info-xzs5x4ah'\n",
      "         cwd: C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_a24b25ba35674d258b6cc8ad478fcdd6\\\n",
      "    Complete output (24 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\__init__.py\", line 18, in <module>\n",
      "        from setuptools.dist import Distribution\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\dist.py\", line 32, in <module>\n",
      "        from setuptools.extern.more_itertools import unique_everseen\n",
      "      File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "      File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "      File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\n",
      "      File \"<frozen importlib._bootstrap>\", line 565, in module_from_spec\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\extern\\__init__.py\", line 52, in create_module\n",
      "        return self.load_module(spec.name)\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\extern\\__init__.py\", line 37, in load_module\n",
      "        __import__(extant)\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\_vendor\\more_itertools\\__init__.py\", line 1, in <module>\n",
      "        from .more import *  # noqa\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\_vendor\\more_itertools\\more.py\", line 5, in <module>\n",
      "        from concurrent.futures import ThreadPoolExecutor\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_a24b25ba35674d258b6cc8ad478fcdd6\\concurrent\\futures\\__init__.py\", line 8, in <module>\n",
      "        from concurrent.futures._base import (FIRST_COMPLETED,\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_a24b25ba35674d258b6cc8ad478fcdd6\\concurrent\\futures\\_base.py\", line 357\n",
      "        raise type(self._exception), self._exception, self._traceback\n",
      "                                   ^\n",
      "    SyntaxError: invalid syntax\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/55/db/97c1ca37edab586a1ae03d6892b6633d8eaa23b23ac40c7e5bbc55423c78/futures-3.0.5.tar.gz#sha256=0542525145d5afc984c88f914a0c85c77527f65946617edb5274f72406f981df (from https://pypi.org/simple/futures/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\anaconda\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\futures_0dddebc4eb4b4e9aadc39be6b6e10bb6\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\futures_0dddebc4eb4b4e9aadc39be6b6e10bb6\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-pip-egg-info-kgguarx1'\n",
      "         cwd: C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_0dddebc4eb4b4e9aadc39be6b6e10bb6\\\n",
      "    Complete output (24 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\__init__.py\", line 18, in <module>\n",
      "        from setuptools.dist import Distribution\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\dist.py\", line 32, in <module>\n",
      "        from setuptools.extern.more_itertools import unique_everseen\n",
      "      File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "      File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "      File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\n",
      "      File \"<frozen importlib._bootstrap>\", line 565, in module_from_spec\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\extern\\__init__.py\", line 52, in create_module\n",
      "        return self.load_module(spec.name)\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\extern\\__init__.py\", line 37, in load_module\n",
      "        __import__(extant)\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\_vendor\\more_itertools\\__init__.py\", line 1, in <module>\n",
      "        from .more import *  # noqa\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\_vendor\\more_itertools\\more.py\", line 5, in <module>\n",
      "        from concurrent.futures import ThreadPoolExecutor\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_0dddebc4eb4b4e9aadc39be6b6e10bb6\\concurrent\\futures\\__init__.py\", line 8, in <module>\n",
      "        from concurrent.futures._base import (FIRST_COMPLETED,\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_0dddebc4eb4b4e9aadc39be6b6e10bb6\\concurrent\\futures\\_base.py\", line 357\n",
      "        raise type(self._exception), self._exception, self._traceback\n",
      "                                   ^\n",
      "    SyntaxError: invalid syntax\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/8d/73/b5fff618482bc06c9711e7cdc0d5d7eb1904d35898f48f2d7f9696b08bef/futures-3.0.4.tar.gz#sha256=19485d83f7bd2151c0aeaf88fbba3ee50dadfb222ffc3b66a344ef4952b782a3 (from https://pypi.org/simple/futures/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\anaconda\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\futures_654b7d2ec92442b1854405d177d36d8a\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\futures_654b7d2ec92442b1854405d177d36d8a\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-pip-egg-info-vxa7x_0e'\n",
      "         cwd: C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_654b7d2ec92442b1854405d177d36d8a\\\n",
      "    Complete output (24 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\__init__.py\", line 18, in <module>\n",
      "        from setuptools.dist import Distribution\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\dist.py\", line 32, in <module>\n",
      "        from setuptools.extern.more_itertools import unique_everseen\n",
      "      File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "      File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "      File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\n",
      "      File \"<frozen importlib._bootstrap>\", line 565, in module_from_spec\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\extern\\__init__.py\", line 52, in create_module\n",
      "        return self.load_module(spec.name)\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\extern\\__init__.py\", line 37, in load_module\n",
      "        __import__(extant)\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\_vendor\\more_itertools\\__init__.py\", line 1, in <module>\n",
      "        from .more import *  # noqa\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\_vendor\\more_itertools\\more.py\", line 5, in <module>\n",
      "        from concurrent.futures import ThreadPoolExecutor\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_654b7d2ec92442b1854405d177d36d8a\\concurrent\\futures\\__init__.py\", line 8, in <module>\n",
      "        from concurrent.futures._base import (FIRST_COMPLETED,\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_654b7d2ec92442b1854405d177d36d8a\\concurrent\\futures\\_base.py\", line 355\n",
      "        raise type(self._exception), self._exception, self._traceback\n",
      "                                   ^\n",
      "    SyntaxError: invalid syntax\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/4c/dc/f9473006d4c9c52d4a4e977173fbcbfb1a8ef3a57e32e885edf994fd4a45/futures-3.0.3.tar.gz#sha256=2fe2342bb4fe8b8e217f0d21b5921cbe5408bf966d9f92025e707e881b198bed (from https://pypi.org/simple/futures/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\anaconda\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\futures_dc745b8473394cb792f4301203025700\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\futures_dc745b8473394cb792f4301203025700\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-pip-egg-info-23krr4le'\n",
      "         cwd: C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_dc745b8473394cb792f4301203025700\\\n",
      "    Complete output (24 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\__init__.py\", line 18, in <module>\n",
      "        from setuptools.dist import Distribution\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\dist.py\", line 32, in <module>\n",
      "        from setuptools.extern.more_itertools import unique_everseen\n",
      "      File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "      File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "      File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\n",
      "      File \"<frozen importlib._bootstrap>\", line 565, in module_from_spec\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\extern\\__init__.py\", line 52, in create_module\n",
      "        return self.load_module(spec.name)\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\extern\\__init__.py\", line 37, in load_module\n",
      "        __import__(extant)\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\_vendor\\more_itertools\\__init__.py\", line 1, in <module>\n",
      "        from .more import *  # noqa\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\_vendor\\more_itertools\\more.py\", line 5, in <module>\n",
      "        from concurrent.futures import ThreadPoolExecutor\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_dc745b8473394cb792f4301203025700\\concurrent\\futures\\__init__.py\", line 8, in <module>\n",
      "        from concurrent.futures._base import (FIRST_COMPLETED,\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_dc745b8473394cb792f4301203025700\\concurrent\\futures\\_base.py\", line 355\n",
      "        raise type(self._exception), self._exception, self._traceback\n",
      "                                   ^\n",
      "    SyntaxError: invalid syntax\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/f8/e7/fc0fcbeb9193ba2d4de00b065e7fd5aecd0679e93ce95a07322b2b1434f4/futures-3.0.2.tar.gz#sha256=dc3fc91508e49e0fd2f8625f0132d16e49c80f882e7e1d565c56b0d5dfbae257 (from https://pypi.org/simple/futures/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\anaconda\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\futures_caabec3691044816af8968332e0a61cf\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\futures_caabec3691044816af8968332e0a61cf\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-pip-egg-info-nqgad_uo'\n",
      "         cwd: C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_caabec3691044816af8968332e0a61cf\\\n",
      "    Complete output (24 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\__init__.py\", line 18, in <module>\n",
      "        from setuptools.dist import Distribution\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\dist.py\", line 32, in <module>\n",
      "        from setuptools.extern.more_itertools import unique_everseen\n",
      "      File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "      File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "      File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\n",
      "      File \"<frozen importlib._bootstrap>\", line 565, in module_from_spec\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\extern\\__init__.py\", line 52, in create_module\n",
      "        return self.load_module(spec.name)\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\extern\\__init__.py\", line 37, in load_module\n",
      "        __import__(extant)\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\_vendor\\more_itertools\\__init__.py\", line 1, in <module>\n",
      "        from .more import *  # noqa\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\_vendor\\more_itertools\\more.py\", line 5, in <module>\n",
      "        from concurrent.futures import ThreadPoolExecutor\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_caabec3691044816af8968332e0a61cf\\concurrent\\futures\\__init__.py\", line 8, in <module>\n",
      "        from concurrent.futures._base import (FIRST_COMPLETED,\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_caabec3691044816af8968332e0a61cf\\concurrent\\futures\\_base.py\", line 355\n",
      "        raise type(self._exception), self._exception, self._traceback\n",
      "                                   ^\n",
      "    SyntaxError: invalid syntax\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/b2/2c/6b6a57379e47031c6f52e625e0e2b8f6702a8d1f61b6e0daee391e82c187/futures-3.0.1.tar.gz#sha256=f78f2ef458639d72a625cf9c7643cf5442bb222ac11c12bcc445c6ad1cd862e2 (from https://pypi.org/simple/futures/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\anaconda\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\futures_9801ba355e774fa5a865d9821b6f1398\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\futures_9801ba355e774fa5a865d9821b6f1398\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-pip-egg-info-5l85ost6'\n",
      "         cwd: C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_9801ba355e774fa5a865d9821b6f1398\\\n",
      "    Complete output (24 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\__init__.py\", line 18, in <module>\n",
      "        from setuptools.dist import Distribution\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\dist.py\", line 32, in <module>\n",
      "        from setuptools.extern.more_itertools import unique_everseen\n",
      "      File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "      File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "      File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\n",
      "      File \"<frozen importlib._bootstrap>\", line 565, in module_from_spec\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\extern\\__init__.py\", line 52, in create_module\n",
      "        return self.load_module(spec.name)\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\extern\\__init__.py\", line 37, in load_module\n",
      "        __import__(extant)\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\_vendor\\more_itertools\\__init__.py\", line 1, in <module>\n",
      "        from .more import *  # noqa\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\_vendor\\more_itertools\\more.py\", line 5, in <module>\n",
      "        from concurrent.futures import ThreadPoolExecutor\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_9801ba355e774fa5a865d9821b6f1398\\concurrent\\futures\\__init__.py\", line 8, in <module>\n",
      "        from concurrent.futures._base import (FIRST_COMPLETED,\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\futures_9801ba355e774fa5a865d9821b6f1398\\concurrent\\futures\\_base.py\", line 354\n",
      "        raise type(self._exception), self._exception, self._traceback\n",
      "                                   ^\n",
      "    SyntaxError: invalid syntax\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/ea/c9/35287369718fc05059e7a9d0d73c53745fe981010b4185b3858e7d46eff1/futures-3.0.0.tar.gz#sha256=d9cd7bb09aa01f0e4940af64c31fbd7045098b7b4354420d7838ea39e8b86ee3 (from https://pypi.org/simple/futures/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\anaconda\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\pywix_1f978d531ff34755af0bf20435033364\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\pywix_1f978d531ff34755af0bf20435033364\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-wheel-h6u7mhuy'\n",
      "       cwd: C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\pywix_1f978d531ff34755af0bf20435033364\\\n",
      "  Complete output (17 lines):\n",
      "  C:\\anaconda\\lib\\distutils\\dist.py:274: UserWarning: Unknown distribution option: 'long_description_markdown_filename'\n",
      "    warnings.warn(msg)\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\pywix_1f978d531ff34755af0bf20435033364\\setup.py\", line 94, in <module>\n",
      "      setup(\n",
      "    File \"C:\\anaconda\\lib\\site-packages\\setuptools\\__init__.py\", line 153, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"C:\\anaconda\\lib\\distutils\\core.py\", line 134, in setup\n",
      "      ok = dist.parse_command_line()\n",
      "    File \"C:\\anaconda\\lib\\distutils\\dist.py\", line 483, in parse_command_line\n",
      "      args = self._parse_command_opts(parser, args)\n",
      "    File \"C:\\anaconda\\lib\\site-packages\\setuptools\\dist.py\", line 1042, in _parse_command_opts\n",
      "      nargs = _Distribution._parse_command_opts(self, parser, args)\n",
      "    File \"C:\\anaconda\\lib\\distutils\\dist.py\", line 555, in _parse_command_opts\n",
      "      raise DistutilsClassError(msg % cmd_class)\n",
      "  distutils.errors.DistutilsClassError: command class <class '__main__.FakeBdistWheelCommand'> must provide 'user_options' attribute (a list of tuples)\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for pywix\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\anaconda\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\pywix_1f978d531ff34755af0bf20435033364\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\pywix_1f978d531ff34755af0bf20435033364\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-record-97eufwrq\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\anaconda\\Include\\pywix'\n",
      "         cwd: C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\pywix_1f978d531ff34755af0bf20435033364\\\n",
      "    Complete output (95 lines):\n",
      "    C:\\anaconda\\lib\\distutils\\dist.py:274: UserWarning: Unknown distribution option: 'long_description_markdown_filename'\n",
      "      warnings.warn(msg)\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib\n",
      "    creating build\\lib\\go_msi\n",
      "    copying go_msi\\_version.py -> build\\lib\\go_msi\n",
      "    copying go_msi\\__init__.py -> build\\lib\\go_msi\n",
      "    creating build\\lib\\pywix\n",
      "    copying pywix\\__init__.py -> build\\lib\\pywix\n",
      "    UPDATING build\\lib\\go_msi/_version.py\n",
      "    set build\\lib\\go_msi/_version.py to '0.2.1'\n",
      "    running install_lib\n",
      "    creating C:\\anaconda\\Lib\\site-packages\\go_msi\n",
      "    copying build\\lib\\go_msi\\_version.py -> C:\\anaconda\\Lib\\site-packages\\go_msi\n",
      "    copying build\\lib\\go_msi\\__init__.py -> C:\\anaconda\\Lib\\site-packages\\go_msi\n",
      "    creating C:\\anaconda\\Lib\\site-packages\\pywix\n",
      "    copying build\\lib\\pywix\\__init__.py -> C:\\anaconda\\Lib\\site-packages\\pywix\n",
      "    byte-compiling C:\\anaconda\\Lib\\site-packages\\go_msi\\_version.py to _version.cpython-39.pyc\n",
      "    byte-compiling C:\\anaconda\\Lib\\site-packages\\go_msi\\__init__.py to __init__.cpython-39.pyc\n",
      "    byte-compiling C:\\anaconda\\Lib\\site-packages\\pywix\\__init__.py to __init__.cpython-39.pyc\n",
      "    running install_egg_info\n",
      "    running egg_info\n",
      "    writing pywix.egg-info\\PKG-INFO\n",
      "    writing dependency_links to pywix.egg-info\\dependency_links.txt\n",
      "    writing requirements to pywix.egg-info\\requires.txt\n",
      "    writing top-level names to pywix.egg-info\\top_level.txt\n",
      "    reading manifest file 'pywix.egg-info\\SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    adding license file 'LICENSE'\n",
      "    writing manifest file 'pywix.egg-info\\SOURCES.txt'\n",
      "    Copying pywix.egg-info to C:\\anaconda\\Lib\\site-packages\\pywix-0.2.1-py3.9.egg-info\n",
      "    running install_scripts\n",
      "    writing list of installed files to 'C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-record-97eufwrq\\install-record.txt'\n",
      "    Forcing web requests to allow TLS v1.2 (Required for requests to Chocolatey.org)\n",
      "    Getting latest version of the Chocolatey package for download.\n",
      "    Not using proxy.\n",
      "    Getting Chocolatey from https://community.chocolatey.org/api/v2/package/chocolatey/1.2.0.\n",
      "    Downloading https://community.chocolatey.org/api/v2/package/chocolatey/1.2.0 to C:\\Users\\UMIKAK~1\\AppData\\Local\\Temp\\chocolatey\\chocoInstall\\chocolatey.zip\n",
      "    Not using proxy.\n",
      "    Extracting C:\\Users\\UMIKAK~1\\AppData\\Local\\Temp\\chocolatey\\chocoInstall\\chocolatey.zip to C:\\Users\\UMIKAK~1\\AppData\\Local\\Temp\\chocolatey\\chocoInstall\n",
      "    Installing Chocolatey on the local machine\n",
      "    WARNING: Setting ChocolateyInstall Environment Variable on USER and not SYSTEM variables.\n",
      "      This is due to either non-administrator install OR the process you are running is not being run as an Administrator.\n",
      "    Creating ChocolateyInstall as an environment variable (targeting 'User')\n",
      "      Setting ChocolateyInstall to 'C:\\ProgramData\\chocolatey'\n",
      "    WARNING: It's very likely you will need to close and reopen your shell\n",
      "      before you can use choco.\n",
      "    Installation of Chocolatey to default folder requires Administrative permissions. Please run from elevated prompt.\n",
      "    Please see https://chocolatey.org/install for details and alternatives if needing to install as a non-administrator.\n",
      "    At C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\chocolatey\\chocoInstall\\tools\\chocolateysetup.psm1:284 char:5\n",
      "    +     throw \"Installation of Chocolatey to default folder requires Admi ...\n",
      "    +     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "        + CategoryInfo          : OperationStopped: (Installation of...-administrator.:String) [], RuntimeException\n",
      "        + FullyQualifiedErrorId : Installation of Chocolatey to default folder requires Administrative permissions. Please\n",
      "        run from elevated prompt. Please see https://chocolatey.org/install for details and alternatives if needing to in\n",
      "      stall as a non-administrator.\n",
      "    \n",
      "    choco : The term 'choco' is not recognized as the name of a cmdlet, function, script file, or operable program. Check\n",
      "    the spelling of the name, or if a path was included, verify that the path is correct and try again.\n",
      "    At line:1 char:1\n",
      "    + choco install -y --allow-empty-checksums go-msi\n",
      "    + ~~~~~\n",
      "        + CategoryInfo          : ObjectNotFound: (choco:String) [], CommandNotFoundException\n",
      "        + FullyQualifiedErrorId : CommandNotFoundException\n",
      "    \n",
      "    choco : The term 'choco' is not recognized as the name of a cmdlet, function, script file, or operable program. Check\n",
      "    the spelling of the name, or if a path was included, verify that the path is correct and try again.\n",
      "    At line:1 char:1\n",
      "    + choco install -y --allow-empty-checksums wixtoolset\n",
      "    + ~~~~~\n",
      "        + CategoryInfo          : ObjectNotFound: (choco:String) [], CommandNotFoundException\n",
      "        + FullyQualifiedErrorId : CommandNotFoundException\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\pywix_1f978d531ff34755af0bf20435033364\\setup.py\", line 94, in <module>\n",
      "        setup(\n",
      "      File \"C:\\anaconda\\lib\\site-packages\\setuptools\\__init__.py\", line 153, in setup\n",
      "        return distutils.core.setup(**attrs)\n",
      "      File \"C:\\anaconda\\lib\\distutils\\core.py\", line 148, in setup\n",
      "        dist.run_commands()\n",
      "      File \"C:\\anaconda\\lib\\distutils\\dist.py\", line 966, in run_commands\n",
      "        self.run_command(cmd)\n",
      "      File \"C:\\anaconda\\lib\\distutils\\dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\pywix_1f978d531ff34755af0bf20435033364\\setup.py\", line 79, in run\n",
      "        find_go_msi()\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\pywix_1f978d531ff34755af0bf20435033364\\go_msi\\__init__.py\", line 67, in find_go_msi\n",
      "        return find_program(['go-msi', 'go-msi.exe'])\n",
      "      File \"C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-install-qy3y3se1\\pywix_1f978d531ff34755af0bf20435033364\\go_msi\\__init__.py\", line 55, in find_program\n",
      "        raise RuntimeError('cannot find {}'.format(os.sep.join(parts)))\n",
      "    RuntimeError: cannot find go-msi\\go-msi.exe\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\anaconda\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\pywix_1f978d531ff34755af0bf20435033364\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Umika Kunta\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qy3y3se1\\\\pywix_1f978d531ff34755af0bf20435033364\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Umika Kunta\\AppData\\Local\\Temp\\pip-record-97eufwrq\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\anaconda\\Include\\pywix' Check the logs for full command output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inltk\n",
      "  Downloading inltk-0.9-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: Pillow in c:\\anaconda\\lib\\site-packages (from inltk) (8.4.0)\n",
      "Collecting fastprogress>=0.1.19\n",
      "  Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\umika kunta\\appdata\\roaming\\python\\python39\\site-packages (from inltk) (21.3)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (from inltk) (2.26.0)\n",
      "Requirement already satisfied: matplotlib in c:\\anaconda\\lib\\site-packages (from inltk) (3.4.3)\n",
      "Collecting aiohttp>=3.5.4\n",
      "  Downloading aiohttp-3.8.3-cp39-cp39-win_amd64.whl (323 kB)\n",
      "Requirement already satisfied: numexpr in c:\\anaconda\\lib\\site-packages (from inltk) (2.7.3)\n",
      "Requirement already satisfied: scipy in c:\\anaconda\\lib\\site-packages (from inltk) (1.7.1)\n",
      "Collecting typing\n",
      "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
      "Requirement already satisfied: pandas in c:\\anaconda\\lib\\site-packages (from inltk) (1.3.4)\n",
      "Collecting nvidia-ml-py3\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "Collecting fastai==1.0.57\n",
      "  Downloading fastai-1.0.57-py3-none-any.whl (233 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\anaconda\\lib\\site-packages (from inltk) (6.0)\n",
      "Requirement already satisfied: bottleneck in c:\\anaconda\\lib\\site-packages (from inltk) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\anaconda\\lib\\site-packages (from inltk) (1.20.3)\n",
      "Collecting spacy>=2.0.18\n",
      "  Downloading spacy-3.4.3-cp39-cp39-win_amd64.whl (11.9 MB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda\\lib\\site-packages (from inltk) (4.10.0)\n",
      "Collecting async-timeout>=3.0.1\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.0-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\anaconda\\lib\\site-packages (from fastai==1.0.57->inltk) (1.13.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\anaconda\\lib\\site-packages (from aiohttp>=3.5.4->inltk) (21.2.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp39-cp39-win_amd64.whl (56 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\anaconda\\lib\\site-packages (from aiohttp>=3.5.4->inltk) (2.0.4)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.10.0-py3-none-any.whl (48 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda\\lib\\site-packages (from spacy>=2.0.18->inltk) (2.11.3)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp39-cp39-win_amd64.whl (96 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.2-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\anaconda\\lib\\site-packages (from spacy>=2.0.18->inltk) (4.62.3)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp39-cp39-win_amd64.whl (481 kB)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (from spacy>=2.0.18->inltk) (58.0.4)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.10\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.5-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\umika kunta\\appdata\\roaming\\python\\python39\\site-packages (from packaging->inltk) (3.0.9)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Collecting typing-extensions>=4.1.0\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests->inltk) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests->inltk) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests->inltk) (1.26.7)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.3-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp39-cp39-win_amd64.whl (7.0 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\umika kunta\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=2.0.18->inltk) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\anaconda\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy>=2.0.18->inltk) (8.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda\\lib\\site-packages (from beautifulsoup4->inltk) (2.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\anaconda\\lib\\site-packages (from jinja2->spacy>=2.0.18->inltk) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\umika kunta\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->inltk) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda\\lib\\site-packages (from matplotlib->inltk) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda\\lib\\site-packages (from matplotlib->inltk) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\umika kunta\\appdata\\roaming\\python\\python39\\site-packages (from cycler>=0.10->matplotlib->inltk) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\anaconda\\lib\\site-packages (from pandas->inltk) (2021.3)\n",
      "Building wheels for collected packages: nvidia-ml-py3, typing\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): started\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): finished with status 'done'\n",
      "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19191 sha256=a6560ee25ca76ef7f9d7cb373af044d4411dabfcd951b0fed93d1f5eea248b91\n",
      "  Stored in directory: c:\\users\\umika kunta\\appdata\\local\\pip\\cache\\wheels\\f6\\d8\\b0\\15cfd7805d39250ac29318105f09b1750683387630d68423e1\n",
      "  Building wheel for typing (setup.py): started\n",
      "  Building wheel for typing (setup.py): finished with status 'done'\n",
      "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26324 sha256=a0f0e7212b9ba22ccfaf99265cfe6d597014b3cd6f7c4991bb595008ce1ba1e6\n",
      "  Stored in directory: c:\\users\\umika kunta\\appdata\\local\\pip\\cache\\wheels\\fa\\17\\1f\\332799f975d1b2d7f9b3f33bbccf65031e794717d24432caee\n",
      "Successfully built nvidia-ml-py3 typing\n",
      "Installing collected packages: typing-extensions, catalogue, srsly, pydantic, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, blis, thinc, spacy-loggers, spacy-legacy, pathy, multidict, langcodes, frozenlist, yarl, torchvision, spacy, nvidia-ml-py3, fastprogress, async-timeout, aiosignal, typing, sentencepiece, fastai, aiohttp, inltk\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 blis-0.7.9 catalogue-2.0.8 confection-0.0.3 cymem-2.0.7 fastai-1.0.57 fastprogress-1.0.3 frozenlist-1.3.3 inltk-0.9 langcodes-3.3.0 multidict-6.0.2 murmurhash-1.0.9 nvidia-ml-py3-7.352.0 pathy-0.10.0 preshed-3.0.8 pydantic-1.10.2 sentencepiece-0.1.97 smart-open-5.2.1 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 torchvision-0.14.0 typer-0.7.0 typing-3.7.4.3 typing-extensions-4.4.0 wasabi-0.10.1 yarl-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza\n",
    "!pip install setuptools\n",
    "!pip install subzero\n",
    "!pip install inltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553,
     "referenced_widgets": [
      "726428bac2554499bea783078bb6593e",
      "0b9f83fe498f40f295d289003ab5ede2",
      "550df98871f34af690e0b2ed26ec82a1",
      "d7e477fe951543b590f2cf6eafaab77a",
      "cfc25ba9617647b4b9d5c75b066d54e6",
      "f05541c100ed484690cd8f1b58ad11a7",
      "b229c205c4de44a1885f836170339816",
      "f3873f326dd646618515defee0898d93"
     ]
    },
    "id": "8HQa7z-8CQ32",
    "outputId": "5c20696d-8d85-40a5-e32e-ce9433745b70"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace36f9c18d24f9fb6007381056be621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 23:15:59 INFO: Downloading these customized packages for language: hi (Hindi)...\n",
      "=============================\n",
      "| Processor       | Package |\n",
      "-----------------------------\n",
      "| tokenize        | hdtb    |\n",
      "| pos             | hdtb    |\n",
      "| lemma           | hdtb    |\n",
      "| pretrain        | hdtb    |\n",
      "| backward_charlm | oscar   |\n",
      "| forward_charlm  | oscar   |\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba3216a40ff45cea2cdbe8942eb84cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-hi/resolve/v1.4.1/models/tokenize/hdtb.pt:   0%|        …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5fd671ebd147bbbbd76f62d97be01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-hi/resolve/v1.4.1/models/pos/hdtb.pt:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05389fb8804d451bb339f8c7624b7c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-hi/resolve/v1.4.1/models/lemma/hdtb.pt:   0%|          |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d3cb99c13547a7a072e8b79bcf796f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-hi/resolve/v1.4.1/models/pretrain/hdtb.pt:   0%|        …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af28a9e48bc24178a1f503838bcc3933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-hi/resolve/v1.4.1/models/backward_charlm/oscar.pt:   0%|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f98bdf0b55744f599259c16967675a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-hi/resolve/v1.4.1/models/forward_charlm/oscar.pt:   0%| …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 23:16:44 INFO: Finished downloading models and saved to C:\\Users\\Umika Kunta\\stanza_resources.\n",
      "2022-12-02 23:16:44 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411e44d287424645a2671296b53384ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 23:16:44 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "| lemma     | hdtb    |\n",
      "=======================\n",
      "\n",
      "2022-12-02 23:16:44 INFO: Use device: cpu\n",
      "2022-12-02 23:16:44 INFO: Loading: tokenize\n",
      "2022-12-02 23:16:44 INFO: Loading: pos\n",
      "2022-12-02 23:16:45 INFO: Loading: lemma\n",
      "2022-12-02 23:16:45 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "SYNSET = []                                                     \n",
    "with open('Synset.txt', encoding= 'unicode_escape') as f:\n",
    "    for line in f:       \n",
    "        x = line.split() \n",
    "        x[3] = x[3].split(':') \n",
    "        SYNSET.append(x)  \n",
    "\n",
    "import stanza                                                   \n",
    "stanza.download('hi', processors='tokenize,pos,lemma')          \n",
    "\n",
    "import csv \n",
    "dataset = \"\" \n",
    "\n",
    "for row in rows: \n",
    "    dataset+=row[1] \n",
    "\n",
    "verbs_content = []  \n",
    "nlp = stanza.Pipeline('hi',processors='tokenize,pos,lemma')\n",
    "\n",
    "doc = nlp(dataset)                                              \n",
    "for sentence in doc.sentences:                                  \n",
    "     for word in sentence.words:                                \n",
    "         if word.upos == 'VERB':                                \n",
    "             verbs_content.append(word.text)                    \n",
    "\n",
    "strongly_negative_words = [] \n",
    "weakly_negative_words = []\n",
    "for line in SUBJCLUE: \n",
    "    totalscore = float(line[2]) - float(line[3])\n",
    "    if(totalscore < -0.25):\n",
    "      for word in line[4]:\n",
    "        strongly_negative_words.append(word)\n",
    "    elif totalscore < 0:  \n",
    "      for word in line[4]: \n",
    "        weakly_negative_words.append(word)\n",
    "        \n",
    "def Getsynset(word):  \n",
    "    syn = [] \n",
    "    flag=0       \n",
    "    syn.append(word)\n",
    "    for line in SYNSET:   \n",
    "        if(line[1]==\"03\"): \n",
    "            for verb in line[3]:\n",
    "                if(word == verb):\n",
    "                    flag = 1 \n",
    "                    break \n",
    "            if(flag):\n",
    "                syn = line[3]\n",
    "                break\n",
    "    return syn \n",
    "\n",
    "s = {}            \n",
    "hlex = []          \n",
    "\n",
    "slist = [\"लड़ना\" , \"मारना\" , \"लूटना\" , \"पीटना\" , \"कूटना\" , \"भेदभाव\" ,\"फोड़ना\", \"तोड़ना\", \"उखाड़ना\" ]    \n",
    "for word in slist:                                                                              \n",
    "  hlex.append(word)                                                                             \n",
    "for word in slist:                                                                              \n",
    "    s = Getsynset(word)                                                                         \n",
    "    for verb1 in s:                                                                             \n",
    "        if verb1 in verbs_content:                                                             \n",
    "            hlex.append(verb1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8xt1a-9EB76",
    "outputId": "f6c4b2ab-88be-46b1-f252-fa8b30ed4807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['बीजेपी ', 'मोदी ', 'माओवादियों ', 'इस्लाम ', 'धमकी ', 'सुरक्षा ', 'धर्म ', 'साले ', 'कुत्ते ', 'कुतिया', 'कुते ', 'कुत्ती', 'कुत्तो', 'कमीना', 'कमीनी', 'साला', 'साली', 'हरामी', 'हरामखोर', 'बहनचोद', 'मादरचोद', 'चूतिया', 'चूत', 'चुत', 'टट्टी', 'नाजायज', 'झांट', 'सुअर', 'बेटीचोद', 'गांड', 'भोसड़ी', 'रन्डी', 'रांड', 'भड़वे', 'लौड़ा', 'लोडे', 'लवड़ा', 'चोर ', 'औलाद ', 'चीन ', 'औकात ', 'चुनौती', 'कश्मीर ', 'ज़ुल्म ', 'मरकज ', 'भारत', 'आतंकवाद', 'इस्लामिक', 'तालिबानी', 'हिन्दू ', 'अर्नब ', 'गद्दारों ', 'कलंकित ', 'तोड़फोड़ ', 'शिवसेना ', 'मंदिर ', 'राम ', 'हिन्दुओं ', 'शूद्र ', 'मुसलमान ', 'विपक्षी ', 'आग ', 'कॉंग्रेस ', 'आतंकवादी ', 'डायन ', 'पलटू ', 'फेंकूँ ', 'पाकिस्तान ', 'जिंदाबाद ', 'आतंकी ', 'आतंकी ', 'आतंकियों ', 'हिंदुस्तान ', 'हिन्दुओं', 'नेता', 'गुलाम ', 'पीओके ', 'आरएसएस ', 'भैंसियो ', 'चमचों ', 'पिल्ला ', 'गधे ', 'तबाह ', 'मुसलमान ', 'मुसलमानों ', 'मौलवी ', 'धर्म ']\n"
     ]
    }
   ],
   "source": [
    "# open themenouns.txt in read\n",
    "themed_nouns = open('themenouns.txt','r', encoding=\"utf8\")\n",
    "themenouns = []                                 \n",
    "for line in themed_nouns:                       \n",
    "    themenouns.append(line.rstrip('\\n'))       \n",
    "# printing the theme nouns list\n",
    "print(themenouns)                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUjlJvTANlud"
   },
   "source": [
    "# Hate speech Detection Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wk_UjwC5IrJI",
    "outputId": "89ba138d-76e7-4930-ed2a-2acde821cbe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['मृत', 'दुर्भाग्यशाली', 'अभागा', 'बदनसीब', 'भाग्यहीन', 'मनहूस', 'बदकिस्मत', 'मंदभाग्य', 'बदक़िस्मत', 'दईमारा', 'कमबख्त', 'कमबख़्त', 'अधन्य', 'अभागी', 'आवासहीन', 'आश्रयहीन', 'गृहहीन', 'गृहविहीन', 'बेघर', 'बेघरबार', 'अगतिक', 'अगेह', 'अनिकेत', 'बदबूदार', 'दुर्गंधपूर्ण', 'दुर्गंधयुक्त', 'दुर्गंधित', 'ढीला', 'अश्लिष्ट', 'असंयुक्त', 'असंयोजित', 'असंबद्ध', 'अलग', 'अजुड़ा', 'अजोड़', 'पृथक्', 'जुदा', 'पृथक', 'अपृक्त', 'पराधीन', 'गुलाम', 'परतंत्र', 'अन्याधीन', 'अपरवश', 'परवश', 'अवश', 'अबस', 'ढीला', 'जड़', 'अचैतन्य', 'जड़त्वयुक्त', 'स्थूल', 'अजैव', 'भौतिक', 'अचेतन', 'चेतनारहित', 'अजीव', 'अनात्म', 'आत्मारहित', 'अफल', 'अफलित', 'फलहीन', 'फलरहित', 'फलविहीन', 'निस्संतान', 'निःसंतान', 'बेऔलाद', 'संतानहीन', 'संतानरहित', 'अऊत', 'अनपत्य', 'भली-भाँति', 'भली_भाँति', 'भरपूर', 'भलीभाँति', 'भली-भांति', 'भली_भांति', 'भलीभांति', 'पंखहीन', 'पक्षरहित', 'अपक्ष', 'अपच्छी', 'अपत्र', 'अतकनीकी', 'ग़ैरतकनीकी', 'गैरतकनीकी', 'तकनीकहीन', 'प्राविधिहीन', 'दुर्गुणी', 'अगुणी', 'अपगुणी', 'ऐबी', 'खोटा', 'विपरीत', 'प्रतिकूल', 'खिलाफ', 'ख़िलाफ़', 'विरुद्ध', 'प्रतीप', 'कुस्वभावी', 'कुबुद्धि', 'दुश्शील', 'बदमिज़ाज', 'बदमिजाज', 'निकम्मा', 'निठल्ला', 'अकर्मण्य', 'निखट्टू', 'अनेरा', 'बेकार', 'फालतू', 'गायताल', 'अकर्मा', 'नालायक', 'ना-लायक', 'अनलायक', 'बेकदर', 'बेक़दर', 'बेक़द्र', 'बेकद्र', 'कठोरहृदय', 'कठोर_हृदय', 'पत्थरदिल', 'संगदिल', 'पाषाण_हृदय', 'हानिकारक', 'क्षतिकारी', 'हानिकर', 'नुक़सानदेह', 'नुकसानदेह', 'हानिप्रद', 'अनर्थकारी', 'क्षतिकर', 'अपमानजनक', 'गूदेदार', 'अप्रसन्नतापूर्वक', 'नाराज़गीपूर्वक', 'नाख़ुशी_से', 'नाराजगीपूर्वक', 'नाखुशी_से', 'पास_में', 'नज़दीक', 'नजदीक', 'निकट_में', 'क़रीब_में', 'समीप', 'निकट', 'पास', 'क़रीब', 'करीब', 'सन्निकट', 'खपड़ा', 'खपरा', 'खपड़ैल', 'खपरैल', 'खप्पर', 'खप्पड़', 'टाइल', 'टॉइल', 'संघर्ष', 'जंग', 'लड़ाई', 'द्वंद्व', 'द्वन्द्व', 'उन्मुक्त', 'आज़ाद', 'आजाद', 'खुला', 'बंधनमुक्त', 'मुक्त', 'अजाद', 'अनिबद्ध', 'मुंच', 'बन्धनमुक्त', 'अबद्ध', 'वीत', 'खीज', 'झुँझलाहट', 'कुढ़न', 'भँड़ास', 'खुंदक', 'खीस', 'अनख', 'भय', 'खौफ', 'ख़ौफ़', 'डर', 'त्रास', 'भीति', 'संत्रास', 'अपभय', 'दहशत', 'खीजना', 'झुँझलाना', 'खिजलाना', 'खिजना', 'कुढ़ना', 'खलना', 'बुरा_लगना', 'अखरना', 'विलाप_करना', 'कलपना', 'बिलखना', 'सोचना', 'दुखी_होना', 'सोंचना', 'अनमनाना', 'उदास_होना', 'अनमनाना', 'चिंताजनक', 'गंभीर', 'चिंतनीय', 'नाजुक', 'शोचनीय', 'सोचनीय', 'नाज़ुक', 'लँगड़ाना', 'लँगड़ाहट', 'लँगड़ापन', 'भचक', 'रुंडित', 'रुण्डित', 'अकारण', 'बेमतलब', 'निष्कारण', 'कारणहीनतः', 'अनिमित्त', 'बिगाड़ना', 'खराब_करना', 'उड़ना', 'गायब_होना', 'छू-मंतर_होना', 'उड़न-छू_होना', 'छूमंतर_होना', 'उड़नछू_होना', 'रौंद', 'रौंदाई', 'हँसी-मज़ाक़', 'हँसी-मजाक', 'हँसी_मज़ाक़', 'हँसी_मजाक', 'खिलवाड़', 'खेलवाड़', 'दिल्लगी', 'ठिठोली', 'ठट्ठा', 'हास्य-परिहास', 'हास-परिहास', 'विनोद', 'कौतुक', 'हँसी', 'परिहास', 'चुहल', 'प्रहसन', 'मज़ाक', 'मजाक', 'हास्य', 'विनोदोक्ति', 'अभिहास', 'धमा-चौकड़ी', 'धमाचौकड़ी', 'उछल-कूद', 'उछलकूद', 'कूद-फाँद', 'कूदफाँद', 'फुसफुसाहट', 'फुसफुस', 'फुस-फुस', 'खुसुफुसाहट', 'खुसुरफुसुर', 'खुसफुसाहट', 'खुसरफुसर', 'खुसर-फुसर', 'खुसुर-फुसुर', 'खुसखुस', 'खुस-खुस', 'खुसपुस', 'खुस_-फुस', 'तेजाबी', 'तेज़ाबी', 'तलछटी', 'अवसादी', 'तलोंछी', 'तलौछी', 'नकलची', 'नक़लची', 'अनुकारी', 'अनुहारक', 'अनुहारी', 'नीचा', 'निम्न', 'अतुंग', 'निभृत', 'अनुच्च', 'अनुन्नत्त', 'अनुन्नत', 'अनूर्ध्व', 'अरक्तक', 'अनीमिक', 'अल्परक्तक', 'अस्थिहीन', 'निराशाजनक', 'जनित', 'जन्य', 'बेहिसाब', 'बेहिसाबी', 'अनलेखा', 'बेपरवाह', 'बेपरवा', 'बिन्दास', 'बेफिक्र', 'बेफ़िक़्र', 'अलगरजी', 'अल्हड़', 'अचिंत', 'अचिन्त', 'उपेक्षक', 'निस्पृह', 'निःस्फृह', 'निर्लोभ', 'लोभहीन', 'लालचहीन', 'अलोभी', 'अस्पृह', 'अतृष्ण', 'लालसारहित', 'तृष्णारहित', 'अपरिवर्तित', 'अनबदला', 'सेवानिवृत्त', 'रिटायर', 'अवकाश_प्राप्त', 'रिटायर्ड', 'तेजहीन', 'निस्तेज', 'बुझा_हुआ', 'आभाहीन', 'कांतिहीन', 'ओजहीन', 'प्रभाहीन', 'फीका', 'बेरौनक', 'अप्रभ', 'प्रभारहित', 'धूलिधूसर', 'धूलि_धूसर', 'धूलिधूसरित', 'धूलि_धूसरित', 'धूसर', 'धूसरा', 'धूलधूसरित', 'तैलीय', 'तेलीय', 'स्निग्ध', 'तेलहा', 'कीचड़दार', 'पंकिल', 'कीचदार', 'कार्दम', 'चिलहला', 'अस्पृश्य', 'अछूत', 'छुतिहा', 'अपरस', 'भ्रमित', 'भ्रांत', 'भ्रान्त', 'क़रीबी', 'नज़दीकी', 'नजदीकी', 'निकटवर्ती', 'समीपी', 'निकटस्थ', 'सन्निकट', 'समीपवर्ती', 'समीपस्थ', 'अपदांतर', 'अपदान्तर', 'शत्रुतापूर्ण', 'दुश्मनाना', 'वैरपूर्ण', 'बैरपूर्ण', 'गरम', 'गर्म', 'उष्ण', 'ताबदार', 'शीतल', 'ठंडा', 'अनुष्ण', 'अतप्त', 'ठण्डा', 'ठंढा', 'ठण्ढा', 'मटमैला', 'ढबैला', 'आग्नेय', 'अदह', 'अदाह्य', 'बेआराम', 'चिरकालीन', 'चिरकालिक', 'दीर्घकालीन', 'दीर्घकालिक', 'दीर्घ-कालीन', 'दीर्घ-कालिक', 'चिर-कालीन', 'चिर-कालिक', 'बिखरा', 'छितराया', 'फैला', 'विकीर्ण', 'बिखरा_हुआ', 'अफ़शाँ', 'अफ़शान', 'अफशान', 'अफशाँ', 'अजेय', 'अजय', 'अजित', 'अपराजेय', 'दुर्जेय', 'अजीत', 'चेतनाहीनता', 'चेतनाशून्यता', 'अचेतनता', 'अचेतना', 'असंज्ञता', 'नाशक', 'नाशी', 'विनाशी', 'विनाशक', 'विनायक', 'विध्वंसक', 'अपध्वंसी', 'अपह', 'अपाय', 'संतोषी', 'संतोषशील', 'छिटपुट', 'छुटपुट', 'छिट-पुट', 'छुट-पुट', 'थोड़ा-बहुत', 'हल्का-फुल्का', 'अवश्य', 'वश_से_परे', 'अराजक', 'अनाथ', 'नाथहीन', 'अनीश', 'अनीस', 'निश्शंक', 'अनाशंकित', 'आशंकाहीन', 'बेखटक', 'बेफ़िक्र', 'संशयहीन', 'अविवाद्य', 'वादातीत', 'असुविधाजनक', 'असुविधापूर्ण', 'नरभक्षी', 'आदमखोर', 'आदमख़ोर', 'मनुष्यभक्षी', 'नौसिखिया', 'कच्चा', 'नव_प्रशिक्षित', 'नौसिखुआ', 'नया', 'नौसिख', 'असिद्ध', 'अनभ्यस्त', 'अपक्व', 'भ्रष्टाचारी', 'मिटनेवाला', 'अशरीरी', 'अशारीरिक', 'रूहानी', 'जड़ित', 'जड़ाऊ', 'जड़ावदार', 'अभिनिविष्ट', 'ख़राब', 'खराब', 'बिगड़ा', 'विकृत', 'विकारग्रस्त', 'अपभ्रंश', 'अपभ्रंशित', 'अबतर', 'विद्रूप', 'अघोषित', 'पराजित', 'परास्त', 'पस्त', 'अपध्वस्त', 'अवज्ञात', 'अभिभवनीय', 'अभिभूत', 'अभिमृष्ट', 'अभिशक्त', 'अनत', 'अनझुका', 'अनमित', 'मौलिक', 'स्वकृत', 'स्वरचित', 'मूल', 'अननुकृत', 'फलाना', 'फलाँ', 'अमुक', 'फलां', 'फला', 'फ़लाँ', 'अमका-धमका', 'फ़लाना', 'अमका', 'म्लान', 'कुम्हलाया', 'मुरझाया', 'रेतीला', 'बालूदार', 'बलुआ', 'सैकत', 'बालुई', 'बलुई', 'बलसुम', 'सिकतिल', 'अखंडनीय', 'अखण्डनीय', 'अभंजनीय', 'अखंड्य', 'अखण्ड्य', 'अत्यावश्यक', 'अति_आवश्यक', 'अपरिहार्य', 'गौण', 'स्वाधीनतः', 'स्वतंत्र_रूप_से', 'अनिर्धारित', 'अनिश्चित', 'वैकल्पिक', 'अप्रतिपन्न', 'अप्रतीयमान', 'सहज', 'आसान', 'सरल', 'सुगम', 'अविकट', 'सहल', 'असावधान', 'लापरवाह', 'अचेत', 'असजग', 'अनचित', 'अनचित्ता', 'ग़फ़लती', 'गफलती', 'अनवहित', 'अनवधान', 'अनाचित', 'गाफिल', 'ग़ाफ़िल', 'अप्रत्यक्ष', 'परोक्ष', 'असीमांकित', 'अक्षुण्ण', 'अक्षुण', 'अविभाजित', 'अखंड', 'अखण्ड', 'अभग्न', 'अविभक्त', 'अखंडित', 'अखण्डित', 'अटूट', 'अभिन्न', 'अखूट', 'अजस्र', 'अनंतरित', 'अनन्तरित', 'अनंतर्हित', 'अनन्तर्हित', 'अनवच्छिन्न', 'अभंग', 'अभङ्ग', 'अभंगी', 'अभङ्गी', 'अभंजन', 'अभञ्जन', 'अभेदनीय', 'अपृथक्', 'अपृथक', 'अभिन्न', 'अभेद', 'अभेव', 'अभेय', 'नीरसता', 'रुचिहीनता', 'अरोचकता', 'ख़ुश्की', 'खुश्की', 'अनरस', 'फीकापन', 'रसहीनता', 'अनर्जित', 'ऋणात्मक', 'ऋण', 'मृत', 'कठिनाई', 'कठिनता', 'मुश्किल', 'बखेड़ा', 'दुरुहता', 'किल्लत', 'क़िल्लत', 'थकाऊ', 'शैक्षिक', 'तालीमी', 'इल्मी', 'सशंकित', 'शंकित', 'सशंक', 'आशंकित', 'आशंकित', 'आशंकापूर्ण', 'फ़िक्रमंद', 'आतंकित', 'त्रस्त', 'भयाक्रांत', 'संत्रस्त', 'भयातुर', 'भयाकुल', 'रोमांचित', 'ग्लानिरहित', 'अकलंक', 'अकलंकित', 'अकलंकी', 'असहमत', 'असम्मत', 'वसाहीन', 'वसामुक्त', 'अमेदस्क', 'अनगढ़', 'अनगढ़ा', 'अनघढ़', 'अनंत', 'असमाप्य', 'अंतहीन', 'अनन्त', 'अन्तहीन', 'अनवसान', 'अयोग्य', 'अनुपयुक्त', 'नाक़ाबिल', 'नाकाबिल', 'अनर्ह', 'नालायक', 'ना-लायक', 'अनलायक', 'अपारग', 'अप्रभु', 'अनम्य', 'अनमनीय', 'कठोर', 'दृढ़', 'अपारिवारिक', 'अपरिवारीय', 'अक्षमाशील', 'अक्षमावान', 'परित्यक्त', 'त्यक्त', 'अपरिगृहीत', 'अपवर्जित', 'अपविद्ध', 'अपास्त', 'अभिनियुक्त', 'विकलांग', 'अपाहिज', 'अंगहीन', 'अपंग', 'अपाहज', 'अपराधी', 'अपराधक', 'अपराध_कर्ता', 'क़सूरवार', 'कसूरवार', 'क़ुसूरवार', 'कुसूरवार', 'गुनहगार', 'दोषी', 'मुजरिम', 'गुनाहगार', 'दंडनीय', 'सहन', 'बर्दाश्त', 'बरदाश्त', 'दुख', 'दुःख', 'तक़लीफ़', 'तकलीफ', 'कष्ट', 'क्लेश', 'परेशानी', 'कोफ़्त', 'कोफ्त', 'अघ', 'अनिर्वृत्ति', 'व्यथित', 'दुःखी', 'दुखी', 'आर्त', 'रंजीदा', 'अनुतपत', 'दिलगीर', 'आनंदपूर्ण', 'आनंदमय', 'उल्लासपूर्ण', 'सानंद', 'अनबूझ', 'अविचारित', 'अचिंतित', 'अचिन्तित', 'अचीता', 'अनचीत', 'अनचीता', 'अनबूझ', 'अनूह', 'औरस', 'ढीला', 'लापता', 'खोया_हुआ', 'गुमशुदा', 'खोया', 'गुम', 'अजूबा', 'प्रियभाषी', 'प्रियंवद', 'प्रियवादी', 'वयस्क', 'बालिग', 'सयाना', 'स्याना', 'अपोगंड', 'बालिग़', 'अप्रधान', 'अप्रमुख', 'आनुषंगिक', 'गौण', 'अविवाहित', 'अनब्याहा', 'कुँआरा', 'कुँवारा', 'क्वाँरा', 'कुंवार', 'कँवारा', 'क्वारा', 'बिनब्याहा', 'अनूढ़', 'अपरिणीत', 'जनाना', 'औरताना', 'स्त्री-सम्बन्धी', 'बचकाना', 'बेमौसम', 'अनार्तव', 'निरर्थक', 'अर्थहीन', 'व्यर्थ', 'फजूल', 'फ़ज़ूल', 'वाहियात', 'बेमतलब_का', 'सारहीन', 'अर्थशून्य', 'अनर्थक', 'अनाह', 'फिजूल', 'फ़िज़ूल', 'वृथा', 'अपार्थ', 'निरामिष', 'अनामिष', 'मांसरहित', 'शस्त्रहीन', 'निहत्था', 'निरस्त्र', 'निश्शस्त्र', 'अबान', 'निरभिमानी', 'अनभिमानी', 'अभिमानरहित', 'गर्वहीन', 'दर्पहीन', 'अदंभी', 'अदर्पी', 'निरहंकारी', 'अहंकारहीन', 'दंभहीन', 'निरहंकर', 'निरहंकृत', 'सीधा', 'अनमद', 'अहंकाररहित', 'गर्वरहित', 'मदशून्य', 'अपरुष', 'अभिमानशून्य', 'अनैतिक', 'नैतिकताहीन', 'अनीतिपूर्ण', 'अनुचित', 'ग़लत', 'गलत', 'नीतिविरुद्ध', 'कम', 'थोड़ा', 'जरा', 'ज़रा', 'अल्प', 'न्यून', 'तनि', 'तनिक', 'कुछ', 'लेश', 'आंशिक', 'अनति', 'अप्रचुर', 'अबहु', 'ऊन', 'तोष', 'अभूयिष्ट', 'अभूरि', 'अमर', 'मृत्यु_विजेता', 'कालजयी', 'कालजीत', 'कालातीत', 'मृत्युंजय', 'अमर्त्य', 'चिरंजीव', 'चिरंजीवी', 'चिरजीवी', 'चिरंजी', 'अंतर्राष्ट्रीय', 'अंतरराष्ट्रीय', 'अंताराष्ट्रीय', 'बहुदेशीय', 'बहुराष्ट्रीय', 'अप्राकृतिक', 'अप्राकृत', 'दिखावटी', 'बनावटी', 'ऊपरी', 'बनौवा', 'स्वाभाविक', 'स्वाभावगत', 'परजीवी', 'लज्जित', 'शर्मिंदा', 'शर्मिन्दा', 'शरमिंदा', 'शरमिन्दा', 'शर्मसार', 'शरमसार', 'शरमाया_हुआ', 'शरमाया', 'अधिकारी', 'अफ़सर', 'अफसर', 'हाकिम', 'निरस्त', 'रद्द', 'निरसित', 'मंसूख', 'मूसलाधार', 'मूसलधार', 'निर्विरोध', 'हरा', 'ठीक-ठाक', 'ठीक_से', 'ठीक-ठाक_से', 'ठीकठाक', 'दुविधाजनक', 'दुबिधाजनक', 'अनिश्चयात्मक', 'भ्रामक', 'संशयात्मक', 'अनुकृत', 'अमौलिक', 'अवैतनिक', 'निर्वेतन', 'अप्रिय', 'अप्रीतिकर', 'नागवार', 'कठिन', 'दुर्गम', 'अगम्य', 'अगम', 'दुरूह', 'दुर्गम्य', 'बीहड़', 'अगत', 'बंक', 'आगम', 'अनाथ', 'यतीम', 'लावारिस', 'बेकस', 'पितृहीन', 'पितृविहीन', 'पिताहीन', 'पितविहीन', 'तातहीन', 'अपितृ', 'अपितृक', 'मातृहीन', 'माताहीन', 'मातृविहीन', 'माताविहीन', 'अमातृक', 'उपद्रवग्रस्त', 'अशांतिपूर्ण', 'अनुशयी', 'पश्चात्तापी', 'अनुशायी', 'अनुशोचत', 'इंद्रियातीत', 'अतींद्रिय', 'अगोचर', 'गोतीत', 'अतीन्द्रिय', 'अप्रत्यक्ष', 'परोक्ष', 'अभौतिक', 'अभ्रमित', 'भ्रमरहित', 'अभ्रांत', 'अभ्रान्त', 'अभरम', 'अप्रमाद', 'निर्भेद्य', 'अभेद्य', 'अप्रवेश्य', 'अगम्य', 'अगत', 'अगम', 'आगम', 'मनोविज्ञानिक', 'मनोवैज्ञानिक', 'मनोविज्ञानी', 'अरोचक', 'नीरस', 'रुचिहीन', 'फीका', 'बेमजा', 'बेमज़ा', 'ख़ुश्क', 'खुश्क', 'अरस', 'असार', 'अप्रासंगिक', 'अप्रासाङ्गिक', 'प्रसंगहीन', 'प्रसङ्गहीन', 'अनुपयुक्त', 'अप्रसंगिक', 'अप्रसङ्गिक', 'अप्रासांगिक', 'अप्रासांङ्गिक', 'ऋणात्मक', 'ऋण', 'अध्यारोपित', 'मिथ्यारोपित', 'असंभव', 'असम्भव', 'अनहोनी', 'असंभावित', 'नामुमकिन', 'असंभावी', 'असंभाव्य', 'अशक्य', 'अघटित', 'अघट', 'अनहोता', 'नामर्द', 'नपुंसक', 'पौरुषहीन', 'पुरषत्वहीन', 'वीर्यहीन', 'अपौरुष', 'अपुरुष', 'शंड', 'विवश', 'बाध्य', 'बेबस', 'मजबूर', 'लाचार', 'जिच', 'जिच्च', 'ज़िच', 'ज़िच्च', 'अवश', 'अबस', 'अनुपस्थित', 'ग़ैरहाज़िर', 'गैरहाजिर', 'ग़ैरमौज़ूद', 'गैरमौजूद', 'अविद्यमान', 'अप्रस्तुत', 'अप्राप्त', 'वंचित', 'महरूम', 'फल', 'फर', 'प्रसून', 'ताल', 'अनुचित', 'असंगत', 'विसंगत', 'गलत', 'ग़लत', 'नामुनासिब', 'अनधिकारी', 'स्वत्वहीन', 'दंडित', 'दण्डित', 'सज़ायाफ़ता', 'सजायाफ्ता', 'अप्रशिक्षित', 'अनसिखा', 'निरुद्विग्न', 'अविकल', 'अव्याकुल', 'प्रशांत', 'शांत', 'निभृत', 'अनाकुल', 'अव्यग्र', 'शान्त', 'प्रशान्त', 'मोपेड', 'निरपवाद', 'अनुपचारित', 'अचिकित्सित', 'अनुपलब्ध', 'अप्राप्य', 'अलभ्य', 'अनधिगम्य', 'अनमिलता', 'अप्राप्त', 'अनियमित', 'नियमरहित', 'बेकायदा', 'बेक़ायदा', 'जलना', 'अग्राह्य', 'अग्रह्य', 'अग्रहणीय', 'असंबंधित', 'असंबद्ध', 'संबंधरहित', 'अटपट', 'अटपटा', 'अनन्वित', 'अप्रसंग', 'असम्बन्धित', 'असम्बद्ध', 'सम्बन्धरहित', 'अबद्ध', 'दुर्व्यवहार', 'दुराचार', 'कुव्यवहार', 'बदसलूकी', 'अनाचार', 'दुराचरण', 'अनाचरण', 'कदाचार', 'कुचाल', 'अपचाल', 'दुष्टाचरण', 'अपकरण', 'अपचार', 'पलायनवाद', 'हवाई_चप्पल', 'स्लीपर', 'अनादरणीय', 'निरादरणीय', 'असम्माननीय', 'अमाननीय', 'अपूजनीय', 'अपूज्य', 'अमान्य', 'खेल', 'तमाशा', 'खेल_प्रदर्शन', 'झाड़ा_फिरना', 'पाखाना_करना', 'पाख़ाना_करना', 'टट्टी_करना', 'दिशा_मैदान_करना', 'करना', 'परिवर्तित_रूप', 'साधनहीन', 'साधनविहीन', 'अज्ञात', 'अनजान', 'अनजाना', 'अविदित', 'अनवगत', 'अपरिचित', 'अपरिगत', 'अनधिगत', 'अनभिज्ञ', 'अज्ञ', 'गुमनाम', 'अजन', 'अजान', 'अजाना', 'बेख़बर', 'बेखबर', 'नावाक़िफ़', 'अनागत', 'अप्रपन्न', 'इच्छाहीनता', 'अनाकांक्षा', 'अनिच्छा', 'अस्पृहा', 'अकामता', 'अनभिलाषिता', 'अनीहा', 'निस्पृहता', 'सौहार्द्र', 'लापरवाही', 'असावधानी', 'सावधानीहीनता', 'अचेतपना', 'चित्तविक्षेप', 'बेपरवाही', 'अलगरजी', 'ग़फ़लत', 'गफलत', 'अनवधान', 'अनवधानता', 'अनाचिती', 'अचिंता', 'अचिन्ता', 'निश्चिंतता', 'निश्चिन्तता', 'निश्चिंतई', 'बेफ़िक्री', 'बेफ़िक़्री', 'बेफिक्री', 'बेफिकरी', 'दिखाना', 'दिखलाना', 'कुरूपता', 'बदसूरती', 'अपाटव', 'दिवालिया', 'दीवालिया', 'दामासाह', 'धर्मनिरपेक्ष', 'अस्वीकृत', 'नामंजूर', 'नामंज़ूर', 'सहमतिहीन', 'सहमति_अप्राप्त', 'और', 'अन्य', 'अवैज्ञानिक', 'विज्ञान_विरूद्ध', 'असांप्रदायिक', 'संवेदनशील', 'कोमलमनस्क', 'संक्रमित', 'अनावासिक', 'प्रतिगामी', 'पश्चगामी', 'पश्चगंता', 'डाँवाडोल', 'डाँवाँडोल', 'घुमक्कड़', 'घुमंतू', 'घुमन्तू', 'पर्यटनप्रिय', 'यायावर', 'जहाँगर्द', 'घूमनेवाला', 'रमता', 'भ्रमणशील', 'भ्रमणीय', 'अतिचारी', 'अध्वगामी', 'ग़श्ती', 'गश्ती', 'घुमना', 'अंधा', 'अन्धा', 'दृष्टिहीन', 'नेत्रहीन', 'अंध', 'अन्ध', 'अँधला', 'अक्षहीन', 'अचक्षु', 'अनयन', 'चक्षुहीन', 'सूरदास', 'वर्णांध', 'वर्णांधता_से_पीड़ित', 'कठिन', 'दुरुह', 'बारीक़', 'बारीक', 'सूक्ष्म', 'अबोधगम्य', 'मुँह-देखा', 'मुंडा', 'केशहीन', 'अलिपिबद्ध', 'अलिखित', 'जीवाणुनाशन', 'विसंक्रमण', 'अनाड़ी', 'अप्रवीण', 'अकुशल', 'अदक्ष', 'अनिपुण', 'अधकचरा', 'अनाप्त', 'अनारी', 'अनैपुण', 'अपटु', 'अपाटव', 'अपात्र', 'ढीला', 'खुरदुरा', 'खुरदरा', 'खुरखुरा', 'कर्कश', 'रूखा', 'रुक्ष', 'रूख', 'छूट', 'चूक', 'ग़फ़लत', 'गफलत', 'असामाजिक', 'गैसीय', 'अनेक', 'कई', 'विविध', 'नाना', 'एकाधिक', 'कतिपय', 'अनेकानेक', 'तमाम', 'अनेग', 'सड़ा', 'स्वरित', 'अबाध्य', 'कुपोषित', 'कुटिल', 'अंटीबाज़', 'अंटीबाज', 'अनार्जव', 'अड़ियल', 'अड़बल', 'अड़ुआ', 'निष्फल', 'असफल', 'विफल', 'व्यर्थ', 'निरर्थक', 'नाकाम', 'अकृतार्थ', 'अपरिणामी', 'परिणामरहित', 'फलरहित', 'अफल', 'अफलित', 'उन्नीस', 'उनीस', 'उन्नीस', 'उनीस', 'अनियत', 'अनिश्चित', 'अनियमित', 'अनिर्दिष्ट', 'अध्रुव', 'सूत्र', 'स्रोत', 'असंदिग्ध', 'संदेहहीन', 'असन्दिग्ध', 'सन्देहहीन', 'तरीक़े_से', 'तरीके_से', 'तरतीब_से', 'क़रीने_से', 'करीने_से', 'व्यवस्थापूर्वक', 'व्यवस्थिततः', 'क़रीनावार', 'करीनावार', 'ठिंगना', 'छोटा', 'बौना', 'नाटा', 'ठिगना', 'वामन', 'वन्य', 'जंगली', 'बनैला', 'आरण्य', 'साउज', 'उलटा-पुलटा', 'उलटा_पुलटा', 'उलटा-पलटा', 'उल्टा-पल्टा', 'उल्टा-पुल्टा', 'उल्टा_पुल्टा', 'कड़ुआ', 'कड़ुवा', 'कड़वा', 'कटु', 'कड़ू', 'सूत्र', 'स्रोत', 'संयमित', 'टूटना', 'अविचारणीय', 'अचिंतनीय', 'अविचार्य', 'असह्य', 'असह', 'असहनीय', 'नागवार', 'ना-गवार', 'अनसहत', 'अप्रसह्य', 'दुरुपयोग', 'महा_युद्ध', 'व्यापक_युद्ध', 'महाभारत', 'महायुद्ध', 'अछूता', 'अश्रुपूर्ण', 'डबडबा', 'डबकौंहाँ', 'डभकौंहाँ', 'अश्रुयुक्त', 'साश्रु', 'अश्रुपूरित', 'अकर्मक', 'अनुपयोगी', 'अनावश्यक', 'उपयोगहीन', 'निरर्थक', 'बेकार', 'व्यर्थ', 'फालतू', 'लंद-फंद', 'अकाज', 'अकारज', 'अकारथ', 'अकारत', 'अनर्थक', 'बेफ़ायदा', 'बेफायदा', 'बेकाम', 'अखंडनीय', 'अखण्डनीय', 'अभंजनीय', 'अखंड्य', 'अखण्ड्य', 'दुष्कर्मी', 'अकृत्यकारी', 'अपकर्मी', 'खल', 'अकर्मी', 'अदृश्य', 'अदृष्टिगोचर', 'लोचनातीत', 'अदृश्यमान', 'विलीन', 'अडीठ', 'अदर्श', 'अनदेखा', 'अदिष्ट', 'अनडीठ', 'अपेख', 'अंतर्हित', 'अन्तर्हित', 'तिरोहित', 'अनिच्छित', 'अचाहा', 'अनचाहा', 'अनपेक्षित', 'अवांछित', 'अनचाहत', 'अनचीत', 'अनचीता', 'अनिष्ट', 'अनभिलषित', 'अनीठ', 'अनीप्सित', 'अजाति', 'अजात', 'जाति_निर्वासित', 'जातिच्युत', 'जाति_बहिष्कृत', 'अजाती', 'अंधाधुंध', 'बेतहाशा', 'मधुमेही', 'मधुमेह_रोगी', 'धब्बेदार', 'दाग़दार', 'दागदार', 'कुपथ्य', 'अपथ्य', 'स्वाधीनतः', 'स्वतंत्र_रूप_से', 'उड़ानहीन', 'उड़नहीन', 'खूनी', 'ख़ूनी']\n",
      "['अच्छा', 'बढ़िया', 'कठिनाई_से', 'जैसे_तैसे', 'मुश्किल_से', 'कठिनतः', 'सड़ा', 'हत', 'वधित', 'मक़तूल', 'दोस्ताना', 'मित्रवत', 'मित्रतापूर्ण', 'मित्रोचित', 'मैत्रीपूर्ण', 'मृतजात', 'असफल', 'नाकामयाब', 'विफल', 'नाकाम', 'निष्फल', 'फौलादी', 'इस्पाती', 'फ़ौलादी', 'इसपाती', 'अभी_भी', 'पूर्व_काल_में', 'पहले', 'कठिनाई_से', 'जैसे_तैसे', 'मुश्किल_से', 'कठिनतः', 'टकराना', 'भिड़ाना', 'खोलना', 'आच्छादित', 'ढँका', 'आवृत्त', 'अपिबद्ध', 'अपिनद्ध', 'अपिहित', 'आच्छन्न', 'उलटना', 'पलटना', 'मासिक', 'अविलंब', 'अविलम्ब', 'छुड़ाना', 'छोड़ाना', 'कृतज्ञ', 'एहसानमंद', 'अहसानमंद', 'आभारी', 'शुक्रगुज़ार', 'शुक्रगुजार', 'उपकृत', 'अनुगृहीत', 'छुड़ाना', 'छोड़ाना', 'उजाड़ना', 'उजाड़_देना', 'नष्ट_करना', 'ख़ाक_करना', 'नाश_करना', 'मिटाना', 'चौपट_करना', 'प्रति', 'कापी', 'कॉपी', 'भयभीत', 'डरा_हुआ', 'डरा', 'कातर', 'आतंकित', 'ख़ौफ़ज़द', 'भयग्रस्त', 'भयान्वित', 'भीत', 'अपत्रस्त', 'दहशतज़दा', 'दहशतजदा', 'सकपकाना', 'चकपकाना', 'भौंचक्का_होना', 'भौचक्का_होना', 'चौंकना', 'पछताना', 'पश्चाताप_करना', 'अछताना-पछताना', 'अफ़सोस_करना', 'अफसोस_करना', 'अपसोसना', 'जलना', 'ईर्ष्या_करना', 'द्वेष_करना', 'डाह_करना', 'कुढ़ना', 'टहलना', 'घूमना', 'विचरना', 'सैर_करना', 'भुजा', 'हाथ', 'बाज़ू', 'हस्त', 'बाँह', 'कर', 'बाहु', 'बाजू', 'झुलसन', 'झुरसन', 'झौंस', 'टूटना', 'उतरना', 'दूभर', 'दुर्भर', 'भीषण', 'भयानक', 'अत्यधिक', 'घनघोर', 'भारी', 'फूँकना', 'फूंकना', 'सँवरना', 'सजना', 'बनना-ठनना', 'शृंगार_करना', 'सजना-धजना', 'जलाना', 'प्रज्वलित_करना', 'जानना', 'समझना', 'बूझना', 'घायल', 'जख्मी', 'ज़ख़्मी', 'आहत', 'क्षत', 'अपचायित', 'चोटिल', 'अभिप्रहत', 'सिसकना', 'सिसकी_भरना', 'सिसकी_लेना', 'सुबकना', 'सुबकी_लेना', 'फूँकना', 'फूंकना', 'जँभाई', 'उबासी', 'जम्हाई', 'जमहाई', 'जम्भाई', 'थोपना', 'ठेलना', 'मत्थे_मढ़ना', 'ठेल_देना', 'डालना', 'लादना', 'भीड़', 'जमघट', 'हुजूम', 'जमाव', 'जमावड़ा', 'भीड़-भाड़', 'भीड़भाड़', 'चहल-पहल', 'चहलपहल', 'मेला', 'मजमा', 'ठट', 'ठठ', 'अंबोह', 'चेतावनी', 'तम्बीह', 'अनुपयोगी', 'अनावश्यक', 'उपयोगहीन', 'निरर्थक', 'बेकार', 'व्यर्थ', 'फालतू', 'लंद-फंद', 'अकाज', 'अकारज', 'अकारथ', 'अकारत', 'अनर्थक', 'बेफ़ायदा', 'बेफायदा', 'बेकाम', 'नारकीय', 'नारकिक', 'नरकीय', 'ईमानदार', 'छलहीन', 'निष्कपट', 'निःकपट', 'रिजु', 'ऋजु', 'दयानतदार', 'सच्चा', 'अपैशुन', 'मोमी', 'मोमिया', 'आसीन', 'विराजमान', 'उपविष्ट', 'अध्यासीन', 'अभिनिविष्ट', 'तरंगित', 'उत्तरंग', 'उर्मिल', 'तरंगी', 'लहरित', 'तरंगायित', 'सक्षम', 'क्षमतावान', 'सामर्थ्यवान', 'क्षमताशाली', 'सामर्थी', 'अपरिवर्तनीय', 'अपरिवर्तनशील', 'अजूना', 'रेडियोधर्मी', 'रेडियोएक्टिव', 'अपारदर्शक', 'अपारदर्शी', 'क्रुद्ध', 'क्रोधित', 'कुपित', 'भामी', 'क्षुब्ध', 'अनखौहा', 'खुला', 'अनावृत', 'अनाच्छादित', 'अनढँका', 'आवरणहीन', 'आवरणरहित', 'अनावेष्टित', 'अपरछन', 'अपरिच्छन्न', 'अप्रच्छन्न', 'विस्फोटक', 'विस्फोटक_पदार्थ', 'कथनीय', 'कथ्य', 'वाच्य', 'अभिभाष्य', 'असघन', 'अघन', 'विरल', 'असंघनित', 'अवचेतन', 'उदारतापूर्वक', 'आत्मतुष्ट', 'आत्मसंतुष्ट', 'आत्मतृप्त', 'नियमी', 'नेमी', 'नियम_पालक', 'आत्मानुशासी', 'विवादित', 'विवादास्पद', 'वादग्रस्त', 'विवादग्रस्त', 'निजाई', 'खरा', 'चोखा', 'सच्चा', 'झबरीला', 'भेंगा', 'तिरपटा', 'कैंचा', 'ऐंचताना', 'सर्गपताली', 'धेरा', 'त्रि-आयामी', 'त्रि-आयामिक', 'त्रिआयामी', 'त्रिआयामिक', 'त्रिविम', 'त्रिविमीय', 'छिछला', 'उथला', 'सतही', 'विद्रोही', 'बाग़ी', 'बागी', 'बलवाई', 'गद्दार', 'ग़द्दार', 'खंडनीय', 'भंजनीय', 'धराशायी', 'भूलुठिंत', 'भू-लुठिंत', 'भूलुण्ठित', 'भू-लुण्ठित', 'अप्रतिष्ठित', 'अप्रतिष्ठ', 'प्रतिष्ठारहित', 'प्रबंधक', 'व्यवस्थापक', 'मैनेजर', 'प्रबंधकर्ता', 'नियामक', 'मुंतजिम', 'प्रबंध_कर्ता', 'प्रबंध-कर्ता', 'प्रबंधकर्त्ता', 'प्रबंध_कर्त्ता', 'प्रबंध-कर्त्ता', 'प्रबन्ध_कर्ता', 'प्रबन्धकर्ता', 'प्रबन्ध_कर्त्ता', 'प्रबन्ध-कर्ता', 'प्रबन्ध-कर्त्ता', 'प्रबन्धकर्त्ता', 'कुरूप', 'बदशक्ल', 'बदसूरत', 'भद्दा', 'असुंदर', 'भदेस', 'भदेसिल', 'भोंडा', 'भौंड़ा', 'अनगढ़', 'अनभिरूप', 'अनरूप', 'अपाटव', 'अबंधुर', 'अबन्धुर', 'अप्रभावित', 'अप्रभान्वित', 'असामयिक', 'अकालिक', 'रोमांचकारी', 'रोमांचक', 'लोमहर्षक', 'अपेक्षणीय', 'अपेक्ष्य', 'सुपरिचित', 'दुबला', 'पतला', 'क्षीण', 'कृशकाय', 'कृश', 'दुबरा', 'अकृत', 'अनकिया', 'विदेशी', 'परदेशी', 'बिदेसी', 'परदेसी', 'विस्मरणीय', 'औपचारिक', 'उपरोक्त', 'उपर्युक्त', 'उल्लिखित', 'उपरिलिखित', 'पूर्वोक्त', 'ऊपर_लिखा', 'रसदार', 'रसपूर्ण', 'रसीला', 'सरस', 'रसवंत', 'रसवान', 'रसाल', 'रसहीन', 'नीरस', 'बेरस', 'त्रिपद', 'त्रिपाद', 'अनुपालन', 'लंबा', 'लंबोतरा', 'बड़ा', 'अपमान', 'अनादर', 'बेइज्जती', 'बे-इज्जती', 'निरादर', 'तिरस्कार', 'हेठी', 'तौहीन', 'तोहीनी', 'जिल्लत', 'फ़ज़ीअत', 'फ़ज़ीहत', 'फजीअत', 'फजीहत', 'अवमान', 'अवमानना', 'अवमानन', 'मानध्वंस', 'मानभंग', 'पराभव', 'बेकदरी', 'भद्द', 'अधिक्षेप', 'अपकर्ष', 'अपचार', 'अपध्वंस', 'बेक़द्री', 'अपहेला', 'अपूजा', 'अप्रतिष्ठा', 'अभिभव', 'आधार', 'अवलंब', 'अवलम्ब', 'आश्रय', 'सहारा', 'पाया', 'अधिकरण', 'जड़', 'अधार', 'अधारी', 'अधिष्ठान', 'कोमलता', 'कोमलताई', 'मुलायमियत', 'मृदुलता', 'नरमीयत', 'नरमी', 'नरमाई', 'नर्मी', 'विचारहीन', 'सतही', 'हल्का', 'स्वर्ण_निर्मित', 'कांचन', 'हैम', 'मध्यम', 'अत्याधुनिक', 'अति-आधुनिक', 'बहुत', 'अधिक', 'ज्यादा', 'ज़्यादा', 'ख़ूब', 'खूब', 'अतिशय', 'अति', 'अगाध', 'अतीव', 'काफ़ी', 'काफी', 'अंबोह', 'अनल्प', 'अनून', 'अन्यून', 'अबेश', 'स्वाभाविक', 'स्वाभावगत', 'शैवाल', 'सेवार', 'तोयशूका', 'तोयवृक्ष', 'जलपृष्ठजा', 'अंबुचामर', 'अम्बुचामर', 'निराशावादी', 'अव्यवस्थित', 'व्यवस्थाहीन', 'अनवस्थ', 'सामान्य', 'आम', 'साधारण', 'कामचलाऊ', 'मामूली', 'अविशिष्ट', 'अविशेष', 'अदिव्य', 'पत्तीदार', 'पर्णी', 'पल्लवित', 'अल्बानियाई', 'अल्बानियावासी', 'अल्बानिया-वासी', 'हृदय-विदारक', 'हृदय_विदारक', 'मार्मिक', 'हृदय_भंजक', 'मर्मभेदी', 'मर्मघाती', 'दिखावटी', 'बनावटी', 'ऊपरी', 'बनौवा', 'दिखावटी', 'बनावटी', 'ऊपरी', 'बनौवा', 'समांतर', 'समानांतर', 'समान्तर', 'समानान्तर', 'अतीत', 'गत', 'भूत', 'व्यतीत', 'बीता', 'गया', 'गुज़रा', 'पिछला', 'विगत', 'पुराना', 'अपेत', 'वर्तमान', 'अजन्मा', 'अजात', 'अनुत्पन्न', 'अनुद्भूत', 'अप्रादुर्भूत', 'अज', 'स्वयंभू', 'स्वयंभु', 'अजन', 'अजन्म', 'अनन्यभव', 'अनागत', 'शांतिपूर्ण', 'शन्तिपूर्ण', 'शांतिमय', 'शान्तिमय', 'अकशेरुकी_जंतु', 'अकशेरुकी_जन्तु', 'अकशेरुकी', 'अकशेरुकी_प्राणी', 'बिल्ली', 'बिलारी', 'बिलाई', 'बिलैया', 'मार्जारी', 'मार्जारीय', 'विड़ाल', 'विराल', 'लार्वा', 'बछेड़ी', 'धनी', 'रोमंथक', 'रोमंथक_प्राणी', 'रोमन्थक', 'रोमन्थक_पशु', 'रोमंथक', 'रोमन्थक', 'लामा', 'विरोधात्मक', 'अधिकारपूर्ण', 'साधिकार', 'बाइख़्तियार', 'बाइख्तियार', 'कंटकारिका', 'अंगुरशफा', 'साग_अंगुर', 'बेलाडोना', 'अवश्यंभावी', 'अटल', 'तय', 'अटलनीय', 'अनिवार्य', 'अबाध्य', 'काज', 'ताश', 'तास', 'कार्ड', 'पत्ता', 'गंजीफा', 'गंजीफ़ा', 'जाँच', 'अप्रधान', 'अप्रमुख', 'आनुषंगिक', 'गौण', 'निजी', 'अपना', 'स्वकीय', 'स्वायत्त', 'आत्म', 'होनहार', 'अवनतिशील', 'पतनशील', 'पतनोन्मुख', 'डायरी', 'दैनंदिनी', 'दैनन्दिनी', 'दैनिकी', 'रोजनामचा', 'रोज़नामचा', 'डाइनमो', 'डाइनेमो', 'ओरी', 'ओलती', 'कढ़ाई', 'कशीदा', 'गुलकारी', 'फुलकारी', 'कढ़ाव', 'असत्यापित', 'अप्रमाणित', 'अनुपपन्न', 'अपरीक्षित', 'फाउंटेन_पेन', 'स्रोतलेखनी', 'हिलाना', 'डुलाना', 'विलोड़न', 'व्यायामशाला', 'व्यायाम_शाला', 'प्रदूषित', 'छँटनी', 'छँटाई', 'हेरोइन', 'लेस', 'लैस', 'मग', 'कलगी', 'कँगूरा', 'कंगूरा', 'जेल', 'जेलख़ाना', 'कारागार', 'बंदी_गृह', 'क़ैदख़ाना', 'कैदखाना', 'कारावास', 'हवालात', 'अस्त्र', 'गुप्ती', 'टरबाइन', 'टरबाईन', 'टरबाइन_मशीन', 'टरबाईन_मशीन', 'निर्धन', 'ग़रीब', 'गरीब', 'दरिद्र', 'दीन', 'धनहीन', 'छुद्र', 'क्षुद्र', 'कंगाल', 'असमृद्ध', 'असंपन्न', 'धनधान्यहीन', 'विपन्न', 'दीनहीन', 'बेचारा', 'मुफ़लिस', 'मुफलिस', 'आजिज़', 'आजिज', 'अकिंचन', 'अनाढ्य', 'असम्पन्न', 'रंक', 'तंगहाल', 'अनिभ्य', 'तंगदस्त', 'विधन', 'बपुरा', 'बापुरा', 'मिसकिन', 'मिस्किन', 'मिसकीन', 'बेकस', 'धनी', 'व्यक्तित्व', 'मानसिक_अवस्था', 'मनोदशा', 'मानसिकता', 'मनःस्थिति', 'मूड', 'मनो_अवस्था', 'मनोवस्था', 'रुखाई', 'रूखापन', 'रुक्षता', 'रुक्षत्व', 'रुखावट', 'रुखाहट', 'अनरस', 'प्रभाव', 'असर', 'छाप', 'रङ्ग', 'तासीर', 'अनुभाव', 'विकार', 'विकृति', 'बिगाड़', 'अपभ्रंश', 'कसर', 'आत्मघाती', 'डुप्लीकेट', 'समरूप', 'धमा-चौकड़ी', 'धमाचौकड़ी', 'उछल-कूद', 'उछलकूद', 'कूद-फाँद', 'कूदफाँद', 'असंतोषजनक', 'असंतोषप्रद', 'गुप्ततः', 'गुपचुप_रूप_से', 'चोरी_छिपे', 'निश्चित', 'निर्धारित', 'नियत', 'ठीक', 'निर्दिष्ट', 'कुडौल', 'कुगठित', 'बेडौल', 'बेढंगा', 'बेढब', 'अनगढ़', 'अनघढ़', 'अपरूप', 'अहस्ताक्षरित', 'बग़ैरदस्तख़ती', 'सेवा-शुश्रूषा', 'शुश्रूषा', 'तीमारदारी', 'बिजैला', 'बीजदार', 'निर्वीज', 'बीजरहित', 'राजद्रोह', 'तारायुक्त', 'तारांकित', 'तारों_भरा', 'तारकिंत', 'टेढ़ा', 'तिरछा', 'तिर्यक', 'आड़ा', 'घुमावदार', 'चक्करदार', 'अटित', 'मस्सा', 'मसा', 'मशक', 'माष', 'अधीन', 'आधीन', 'असहाय', 'निस्सहाय', 'बेसहारा', 'निराश्रित', 'निराश्रय', 'अनाश्रित', 'आश्रयहीन', 'अपाश्रय', 'निरवलंब', 'अवलंबहीन', 'अवलंबनहीन', 'निरवलम्ब', 'अवलम्बहीन', 'अवलम्बनहीन', 'बेचारा', 'बपुरा', 'बापुरा', 'अनवस्थित', 'अनाथ', 'निरवलम्ब', 'अनाश्रित', 'निःसहाय', 'बेकस', 'नज़र', 'नजर', 'कुदृष्टि', 'बुरी_नज़र', 'डीठ', 'चुप्पा', 'घुन्ना', 'अनालाप', 'बुनाई', 'बिनाई', 'बुनावट', 'ध्रुव_तारा', 'ध्रुव', 'सुकुमार', 'कोमल', 'नाजुक', 'नाज़ुक', 'कोमलांग', 'मृदुल', 'फूलपान', 'रुआँसा', 'रोवासा', 'रोआँसा', 'रोनी', 'निर्णीत', 'निर्णित', 'निपटा_हुआ', 'तय', 'तयशुदा', 'आच्छादित', 'ढँका', 'आवृत्त', 'अपिबद्ध', 'अपिनद्ध', 'अपिहित', 'आच्छन्न', 'जरायुज', 'गर्भज', 'पिंडज', 'पिण्डज', 'गरम', 'गर्म', 'उष्ण', 'ताबदार', 'शीतल', 'ठंडा', 'अनुष्ण', 'अतप्त', 'ठण्डा', 'ठंढा', 'ठण्ढा', 'ऊपर', 'कम', 'थोड़ा', 'जरा', 'ज़रा', 'अल्प', 'न्यून', 'तनि', 'तनिक', 'कुछ', 'लेश', 'आंशिक', 'अनति', 'अप्रचुर', 'अबहु', 'ऊन', 'तोष', 'अभूयिष्ट', 'अभूरि', 'समाधानित', 'निपटा_हुआ', 'सुलझा_हुआ', 'सुलझा', 'वर्षा_कालीन', 'बरसाती', 'बारानी', 'निर्जल', 'निर्जला', 'पहिएदार', 'पहियेदार', 'नंगे_पैर']\n",
      "['लड़ना', 'मारना', 'लूटना', 'पीटना', 'कूटना', 'भेदभाव', 'फोड़ना', 'तोड़ना', 'उखाड़ना', 'लड़ना', 'मारना', 'उखाड़ना']\n",
      "['बीजेपी ', 'मोदी ', 'माओवादियों ', 'इस्लाम ', 'धमकी ', 'सुरक्षा ', 'धर्म ', 'साले ', 'कुत्ते ', 'कुतिया', 'कुते ', 'कुत्ती', 'कुत्तो', 'कमीना', 'कमीनी', 'साला', 'साली', 'हरामी', 'हरामखोर', 'बहनचोद', 'मादरचोद', 'चूतिया', 'चूत', 'चुत', 'टट्टी', 'नाजायज', 'झांट', 'सुअर', 'बेटीचोद', 'गांड', 'भोसड़ी', 'रन्डी', 'रांड', 'भड़वे', 'लौड़ा', 'लोडे', 'लवड़ा', 'चोर ', 'औलाद ', 'चीन ', 'औकात ', 'चुनौती', 'कश्मीर ', 'ज़ुल्म ', 'मरकज ', 'भारत', 'आतंकवाद', 'इस्लामिक', 'तालिबानी', 'हिन्दू ', 'अर्नब ', 'गद्दारों ', 'कलंकित ', 'तोड़फोड़ ', 'शिवसेना ', 'मंदिर ', 'राम ', 'हिन्दुओं ', 'शूद्र ', 'मुसलमान ', 'विपक्षी ', 'आग ', 'कॉंग्रेस ', 'आतंकवादी ', 'डायन ', 'पलटू ', 'फेंकूँ ', 'पाकिस्तान ', 'जिंदाबाद ', 'आतंकी ', 'आतंकी ', 'आतंकियों ', 'हिंदुस्तान ', 'हिन्दुओं', 'नेता', 'गुलाम ', 'पीओके ', 'आरएसएस ', 'भैंसियो ', 'चमचों ', 'पिल्ला ', 'गधे ', 'तबाह ', 'मुसलमान ', 'मुसलमानों ', 'मौलवी ', 'धर्म ']\n"
     ]
    }
   ],
   "source": [
    "print(strongly_negative_words)\n",
    "print(weakly_negative_words) \n",
    "print(hlex)                   \n",
    "print(themenouns)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWR9UfCpYFIb"
   },
   "source": [
    "## Calculating Scores without Subjective Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEYAJ25UZBgo"
   },
   "source": [
    "### Only Semantic feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQIDn-pKNn4V",
    "outputId": "6212281d-7fea-4f3d-8e89-1afb3c246648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of rows: 811\n",
      "No Hate: 357\n",
      "Actual no hate: 207\n",
      "Weak Hate: 454\n",
      "Actual weak hate: 112\n",
      "Strong Hate: 0\n",
      "Actual strong hate: 0\n",
      "Precision: 0.3933415536374846\n",
      "Recall: 0.7799511002444988\n",
      "F-score: 0.5229508196721312\n"
     ]
    }
   ],
   "source": [
    "for row in rows: \n",
    "    strongcount = 0\n",
    "    hlexcount = 0 \n",
    "    weakcount = 0          \n",
    "    themecount = 0                  \n",
    "    if any([word in row[1] for word in strongly_negative_words]):\n",
    "      strongcount += 1\n",
    "    # if any([word in row[1] for word in hlex]): \n",
    "    if any([word in row[1] for word in weakly_negative_words]):\n",
    "      weakcount += 1                                       \n",
    "    if strongcount >= 2:        \n",
    "        row.append(\"strongly hateful\")    \n",
    "    elif strongcount == 1:          \n",
    "      if hlexcount >= 1 or themecount >= 1: \n",
    "        row.append(\"strongly hateful\")\n",
    "      else:  \n",
    "        row.append(\"weakly hateful\")\n",
    "    elif strongcount == 0:   \n",
    "      if themecount >= 1 and hlexcount >= 1:\n",
    "        row.append(\"strongly hateful\") \n",
    "      elif themecount >=1 and weakcount >= 1:                         \n",
    "        row.append(\"weakly hateful\") \n",
    "      elif hlexcount == 1: \n",
    "        row.append(\"weakly hateful\") \n",
    "      else:   \n",
    "        row.append(\"No Hate\")\n",
    "\n",
    "total_rows = [row for row in rows]\n",
    "\n",
    "no_hate_rows = [row for row in rows if row[4] == \"No Hate\"]\n",
    "\n",
    "correct_no_hate_rows = [row for row in no_hate_rows if row[4] == \"No Hate\" and row[2] == \"non-hostile\"]\n",
    "\n",
    "weak_hate_rows = [row for row in rows if row[4] == \"weakly hateful\"]\n",
    "\n",
    "correct_weak_hate_rows = [row for row in weak_hate_rows if row[4] == \"weakly hateful\" and (row[2] == \"fake\" or row[2] == \"defamation\")]\n",
    "\n",
    "strong_hate_rows = [row for row in rows if row[4] == \"strongly hateful\"]\n",
    "\n",
    "correct_strong_hate_rows = [row for row in strong_hate_rows if row[4] == \"strongly hateful\" and row[2] != \"non-hostile\" and row[2] != \"fake\" and row[2] != \"defamation\"]\n",
    "\n",
    "false_neg_no_hate = [row for row in no_hate_rows if row[2] == \"non-hostile\" and row[4] != \"No Hate\"]\n",
    "\n",
    "false_neg_weak_hate = [row for row in weak_hate_rows if row[2] == \"fake\" or row[2] == \"defamation\" and row[4] != \"weakly hateful\"]\n",
    "\n",
    "false_neg_strong_hate = [row for row in strong_hate_rows if row[2] != \"non-hostile\" and row[2] != \"fake\" and row[2] != \"defamation\" and row[4] != \"strongly hateful\"]\n",
    "\n",
    "precision = (len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows))/(len(no_hate_rows)+len(strong_hate_rows)+len(weak_hate_rows))\n",
    "\n",
    "recall = (len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows))/(len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows)+len(false_neg_no_hate)+len(false_neg_strong_hate)+len(false_neg_weak_hate))\n",
    "\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print(\"Total no. of rows: {}\".format(len(total_rows)))\n",
    "print(\"No Hate: {}\".format(len(no_hate_rows))) \n",
    "print(\"Actual no hate: {}\".format(len(correct_no_hate_rows)))\n",
    "print(\"Weak Hate: {}\".format(len(weak_hate_rows)))     \n",
    "print(\"Actual weak hate: {}\".format(len(correct_weak_hate_rows)))\n",
    "print(\"Strong Hate: {}\".format(len(strong_hate_rows))) \n",
    "print(\"Actual strong hate: {}\".format(len(correct_strong_hate_rows)))\n",
    "\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "print(\"F-score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hqy7pvksZLyT"
   },
   "source": [
    "### Semantic + Hate Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmOcHTyUmB4P",
    "outputId": "c5a55ff2-1194-4f71-b505-ed557f8da634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of rows: 811\n",
      "No Hate: 355\n",
      "Actual no hate: 206\n",
      "Weak Hate: 455\n",
      "Actual weak hate: 112\n",
      "Strong Hate: 1\n",
      "Actual strong hate: 1\n",
      "Precision: 0.3933415536374846\n",
      "Recall: 0.7799511002444988\n",
      "F-score: 0.5229508196721312\n"
     ]
    }
   ],
   "source": [
    "for row in rows:\n",
    "    strongcount = 0   \n",
    "    hlexcount = 0\n",
    "    weakcount = 0\n",
    "    themecount = 0                  \n",
    "    if any([word in row[1] for word in strongly_negative_words]):\n",
    "      strongcount += 1 \n",
    "    if any([word in row[1] for word in hlex]):\n",
    "      hlexcount += 1\n",
    "    if any([word in row[1] for word in weakly_negative_words]):\n",
    "      weakcount += 1                                                 \n",
    "    if strongcount >= 2:                                             \n",
    "        row[4] = \"strongly hateful\"                               \n",
    "    elif strongcount == 1:                               \n",
    "      if hlexcount >= 1 or themecount >= 1:\n",
    "        row[4] = \"strongly hateful\" \n",
    "      else:                         \n",
    "        row[4] = \"weakly hateful\"         \n",
    "    elif strongcount == 0:     \n",
    "      if themecount >= 1 and hlexcount >= 1:\n",
    "        row[4] = \"strongly hateful\"\n",
    "      elif themecount >=1 and weakcount >= 1:\n",
    "        row[4] = \"weakly hateful\"\n",
    "      elif hlexcount == 1: \n",
    "        row[4] = \"weakly hateful\"\n",
    "      else: \n",
    "        row[4] = \"No Hate\" \n",
    "\n",
    "\n",
    "total_rows = [row for row in rows]\n",
    "\n",
    "no_hate_rows = [row for row in rows if row[4] == \"No Hate\"]\n",
    "\n",
    "correct_no_hate_rows = [row for row in no_hate_rows if row[4] == \"No Hate\" and row[2] == \"non-hostile\"]\n",
    "\n",
    "weak_hate_rows = [row for row in rows if row[4] == \"weakly hateful\"]\n",
    "\n",
    "correct_weak_hate_rows = [row for row in weak_hate_rows if row[4] == \"weakly hateful\" and (row[2] == \"fake\" or row[2] == \"defamation\")]\n",
    "\n",
    "strong_hate_rows = [row for row in rows if row[4] == \"strongly hateful\"]\n",
    "\n",
    "correct_strong_hate_rows = [row for row in strong_hate_rows if row[4] == \"strongly hateful\" and row[2] != \"non-hostile\" and row[2] != \"fake\" and row[2] != \"defamation\"]\n",
    "\n",
    "false_neg_no_hate = [row for row in no_hate_rows if row[2] == \"non-hostile\" and row[4] != \"No Hate\"]\n",
    "\n",
    "false_neg_weak_hate = [row for row in weak_hate_rows if row[2] == \"fake\" or row[2] == \"defamation\" and row[4] != \"weakly hateful\"]\n",
    "\n",
    "false_neg_strong_hate = [row for row in strong_hate_rows if row[2] != \"non-hostile\" and row[2] != \"fake\" and row[2] != \"defamation\" and row[4] != \"strongly hateful\"]\n",
    "\n",
    "\n",
    "precision = (len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows))/(len(no_hate_rows)+len(strong_hate_rows)+len(weak_hate_rows))\n",
    "\n",
    "recall = (len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows))/(len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows)+len(false_neg_no_hate)+len(false_neg_strong_hate)+len(false_neg_weak_hate))\n",
    "\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print(\"Total no. of rows: {}\".format(len(total_rows)))                  \n",
    "print(\"No Hate: {}\".format(len(no_hate_rows)))                          \n",
    "print(\"Actual no hate: {}\".format(len(correct_no_hate_rows)))          \n",
    "print(\"Weak Hate: {}\".format(len(weak_hate_rows)))                       \n",
    "print(\"Actual weak hate: {}\".format(len(correct_weak_hate_rows)))\n",
    "print(\"Strong Hate: {}\".format(len(strong_hate_rows)))\n",
    "print(\"Actual strong hate: {}\".format(len(correct_strong_hate_rows)))\n",
    "# printing precision\n",
    "print(\"Precision: {}\".format(precision))\n",
    "# printing recall\n",
    "print(\"Recall: {}\".format(recall))\n",
    "# printing f1 score\n",
    "print(\"F-score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbEg764EZSlI"
   },
   "source": [
    "### Semantic + Hate Lexicon + Thematic Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vG1O1mUVmCL8",
    "outputId": "c4c29c0f-daa6-4ff7-fbb7-eaa8aa577bc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of rows: 811\n",
      "No Hate: 293\n",
      "Actual no hate: 185\n",
      "Weak Hate: 344\n",
      "Actual weak hate: 85\n",
      "Strong Hate: 174\n",
      "Actual strong hate: 75\n",
      "Precision: 0.4254007398273736\n",
      "Recall: 0.843520782396088\n",
      "F-score: 0.5655737704918032\n"
     ]
    }
   ],
   "source": [
    "for row in rows:                \n",
    "    strongcount = 0                            \n",
    "    hlexcount = 0                \n",
    "    weakcount = 0                  \n",
    "    themecount = 0                        \n",
    "    if any([word in row[1] for word in strongly_negative_words]):\n",
    "      strongcount += 1                                              \n",
    "    if any([word in row[1] for word in hlex]): \n",
    "      hlexcount += 1                           \n",
    "    if any([word in row[1] for word in weakly_negative_words]):\n",
    "      weakcount += 1   \n",
    "    if any([word in row[1] for word in themenouns]):\n",
    "      themecount += 1  \n",
    "\n",
    "    if strongcount >= 2: \n",
    "        row[4] = \"strongly hateful\" \n",
    "    elif strongcount == 1:\n",
    "      if hlexcount >= 1 or themecount >= 1:\n",
    "        row[4] = \"strongly hateful\"\n",
    "      else:  \n",
    "        row[4] = \"weakly hateful\" \n",
    "    elif strongcount == 0: \n",
    "      if themecount >= 1 and hlexcount >= 1: \n",
    "        row[4] = \"strongly hateful\"\n",
    "      elif themecount >=1 and weakcount >= 1:\n",
    "        row[4] = \"weakly hateful\" \n",
    "      elif hlexcount == 1:\n",
    "        row[4] = \"weakly hateful\"\n",
    "      else:                    \n",
    "        row[4] = \"No Hate\" \n",
    "\n",
    "\n",
    "total_rows = [row for row in rows]\n",
    "\n",
    "no_hate_rows = [row for row in rows if row[4] == \"No Hate\"]\n",
    "\n",
    "correct_no_hate_rows = [row for row in no_hate_rows if row[4] == \"No Hate\" and row[2] == \"non-hostile\"]\n",
    "\n",
    "weak_hate_rows = [row for row in rows if row[4] == \"weakly hateful\"]\n",
    "\n",
    "correct_weak_hate_rows = [row for row in weak_hate_rows if row[4] == \"weakly hateful\" and (row[2] == \"fake\" or row[2] == \"defamation\")]\n",
    "\n",
    "strong_hate_rows = [row for row in rows if row[4] == \"strongly hateful\"]\n",
    "\n",
    "correct_strong_hate_rows = [row for row in strong_hate_rows if row[4] == \"strongly hateful\" and row[2] != \"non-hostile\" and row[2] != \"fake\" and row[2] != \"defamation\"]\n",
    "\n",
    "false_neg_no_hate = [row for row in no_hate_rows if row[2] == \"non-hostile\" and row[4] != \"No Hate\"]\n",
    "\n",
    "false_neg_weak_hate = [row for row in weak_hate_rows if row[2] == \"fake\" or row[2] == \"defamation\" and row[4] != \"weakly hateful\"]\n",
    "\n",
    "false_neg_strong_hate = [row for row in strong_hate_rows if row[2] != \"non-hostile\" and row[2] != \"fake\" and row[2] != \"defamation\" and row[4] != \"strongly hateful\"]\n",
    "\n",
    "\n",
    "precision = (len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows))/(len(no_hate_rows)+len(strong_hate_rows)+len(weak_hate_rows))\n",
    "\n",
    "recall = (len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows))/(len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows)+len(false_neg_no_hate)+len(false_neg_strong_hate)+len(false_neg_weak_hate))\n",
    "\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print(\"Total no. of rows: {}\".format(len(total_rows)))               \n",
    "print(\"No Hate: {}\".format(len(no_hate_rows)))     \n",
    "print(\"Actual no hate: {}\".format(len(correct_no_hate_rows)))\n",
    "print(\"Weak Hate: {}\".format(len(weak_hate_rows)))     \n",
    "print(\"Actual weak hate: {}\".format(len(correct_weak_hate_rows)))\n",
    "print(\"Strong Hate: {}\".format(len(strong_hate_rows))) \n",
    "print(\"Actual strong hate: {}\".format(len(correct_strong_hate_rows)))\n",
    "#printing precision\n",
    "print(\"Precision: {}\".format(precision))\n",
    "#printing recall\n",
    "print(\"Recall: {}\".format(recall))\n",
    "#printing f1 score\n",
    "print(\"F-score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lWHjD3FYOoQ"
   },
   "source": [
    "## Calculating Scores with Subjective Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "erIbFiW6BR-7",
    "outputId": "4a8cf0cd-6c5d-4d1a-bbce-11e876efd7a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Subjective Sentences: \n",
      "355\n"
     ]
    }
   ],
   "source": [
    "counter = 0       \n",
    "subj_rows = []\n",
    "for row in rows: \n",
    "  if row[3] <= -0.5 or row[3] >= 1: \n",
    "    subj_rows.append(row)\n",
    "    counter += 1\n",
    "\n",
    "print(\"Number of Subjective Sentences: \")\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-g57Y2EZa-o"
   },
   "source": [
    "### Semantic feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9HwRDrzLOwJ",
    "outputId": "f90a1305-9602-4ac0-a3a7-65c2ac9e4770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of rows: 811\n",
      "No Hate: 479\n",
      "Actual no hate: 282\n",
      "Weak Hate: 332\n",
      "Actual weak hate: 86\n",
      "Strong Hate: 0\n",
      "Actual strong hate: 0\n",
      "Precision: 0.45376078914919854\n",
      "Recall: 0.8498845265588915\n",
      "F-score: 0.5916398713826366\n"
     ]
    }
   ],
   "source": [
    "for row in rows: \n",
    "  if row[3] <= -0.5 or row[3] >= 1: \n",
    "    strongcount = 0                   \n",
    "    hlexcount = 0          \n",
    "    weakcount = 0                                     \n",
    "    themecount = 0         \n",
    "    if any([word in row[1] for word in strongly_negative_words]): \n",
    "      strongcount += 1                      \n",
    "                  \n",
    "    if any([word in row[1] for word in weakly_negative_words]): \n",
    "      weakcount += 1                                   \n",
    "   \n",
    "    if strongcount >= 2:                                           \n",
    "        row.append(\"strongly hateful\")                           \n",
    "    elif strongcount == 1:                                        \n",
    "      if hlexcount >= 1 or themecount >= 1:                    \n",
    "        row.append(\"strongly hateful\")                \n",
    "      else:                                                 \n",
    "        row.append(\"weakly hateful\")                     \n",
    "    elif strongcount == 0:                               \n",
    "      if themecount >= 1 and hlexcount >= 1:             \n",
    "        row.append(\"strongly hateful\")                        \n",
    "      elif themecount >=1 and weakcount >= 1:             \n",
    "        row.append(\"weakly hateful\")                         \n",
    "      elif hlexcount == 1:                                 \n",
    "        row.append(\"weakly hateful\")                    \n",
    "      else:                                                      \n",
    "        row.append(\"No Hate\")                  \n",
    "  else:                                        \n",
    "     row.append(\"No Hate\")                     \n",
    "\n",
    "\n",
    "total_rows = [row for row in rows]\n",
    "\n",
    "no_hate_rows = [row for row in rows if row[5] == \"No Hate\"]\n",
    "correct_no_hate_rows = [row for row in no_hate_rows if row[5] == \"No Hate\" and row[2] == \"non-hostile\"]\n",
    "weak_hate_rows = [row for row in rows if row[5] == \"weakly hateful\"]\n",
    "correct_weak_hate_rows = [row for row in weak_hate_rows if row[5] == \"weakly hateful\" and (row[2] == \"fake\" or row[2] == \"defamation\")]\n",
    "strong_hate_rows = [row for row in rows if row[5] == \"strongly hateful\"]\n",
    "correct_strong_hate_rows = [row for row in strong_hate_rows if row[5] == \"strongly hateful\" and row[2] != \"non-hostile\" and row[2] != \"fake\" and row[2] != \"defamation\"]\n",
    "\n",
    "false_neg_no_hate = [row for row in no_hate_rows if row[2] == \"non-hostile\" and row[5] != \"No Hate\"]\n",
    "false_neg_weak_hate = [row for row in weak_hate_rows if row[2] == \"fake\" or row[2] == \"defamation\" and row[5] != \"weakly hateful\"]\n",
    "false_neg_strong_hate = [row for row in strong_hate_rows if row[2] != \"non-hostile\" and row[2] != \"fake\" and row[2] != \"defamation\" and row[5] != \"strongly hateful\"]\n",
    "\n",
    "precision = (len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows))/(len(no_hate_rows)+len(strong_hate_rows)+len(weak_hate_rows))\n",
    "recall = (len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows))/(len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows)+len(false_neg_no_hate)+len(false_neg_strong_hate)+len(false_neg_weak_hate))\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print(\"Total no. of rows: {}\".format(len(total_rows)))\n",
    "print(\"No Hate: {}\".format(len(no_hate_rows)))\n",
    "print(\"Actual no hate: {}\".format(len(correct_no_hate_rows)))\n",
    "print(\"Weak Hate: {}\".format(len(weak_hate_rows)))\n",
    "print(\"Actual weak hate: {}\".format(len(correct_weak_hate_rows)))\n",
    "print(\"Strong Hate: {}\".format(len(strong_hate_rows)))\n",
    "print(\"Actual strong hate: {}\".format(len(correct_strong_hate_rows)))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "print(\"F-score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXxMHvFpZdaF"
   },
   "source": [
    "### Semantic + Hate Lexicon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wBRGSWQrL2r",
    "outputId": "c455637c-c486-4a6a-c8e9-52e9cb684c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of rows: 811\n",
      "No Hate: 479\n",
      "Actual no hate: 282\n",
      "Weak Hate: 331\n",
      "Actual weak hate: 86\n",
      "Strong Hate: 1\n",
      "Actual strong hate: 1\n",
      "Precision: 0.45499383477188654\n",
      "Recall: 0.8502304147465438\n",
      "F-score: 0.5927710843373494\n"
     ]
    }
   ],
   "source": [
    "for row in rows:                                                   \n",
    "  if row[3] <= -0.5 or row[3] >= 1:\n",
    "    strongcount = 0 \n",
    "    hlexcount = 0\n",
    "    weakcount = 0                \n",
    "    themecount = 0\n",
    "    if any([word in row[1] for word in strongly_negative_words]):\n",
    "      strongcount += 1                                          \n",
    "    if any([word in row[1] for word in hlex]):\n",
    "      hlexcount += 1 \n",
    "    if any([word in row[1] for word in weakly_negative_words]):\n",
    "      weakcount += 1                 \n",
    " \n",
    "    if strongcount >= 2: \n",
    "        row[5] = \"strongly hateful\"\n",
    "    elif strongcount == 1:\n",
    "      if hlexcount >= 1 or themecount >= 1:\n",
    "        row[5] = \"strongly hateful\" \n",
    "      else: \n",
    "        row[5] = \"weakly hateful\"       \n",
    "    elif strongcount == 0:\n",
    "      if themecount >= 1 and hlexcount >= 1:\n",
    "        row[5] = \"strongly hateful\" \n",
    "      elif themecount >=1 and weakcount >= 1:\n",
    "        row[5] = \"weakly hateful\"\n",
    "      elif hlexcount == 1:\n",
    "        row[5] = \"weakly hateful\" \n",
    "      else: \n",
    "        row[5] = \"No Hate\"\n",
    "\n",
    "\n",
    "total_rows = [row for row in rows]\n",
    "\n",
    "no_hate_rows = [row for row in rows if row[5] == \"No Hate\"]\n",
    "correct_no_hate_rows = [row for row in no_hate_rows if row[5] == \"No Hate\" and row[2] == \"non-hostile\"]\n",
    "weak_hate_rows = [row for row in rows if row[5] == \"weakly hateful\"]\n",
    "correct_weak_hate_rows = [row for row in weak_hate_rows if row[5] == \"weakly hateful\" and (row[2] == \"fake\" or row[2] == \"defamation\")]\n",
    "strong_hate_rows = [row for row in rows if row[5] == \"strongly hateful\"]\n",
    "correct_strong_hate_rows = [row for row in strong_hate_rows if row[5] == \"strongly hateful\" and row[2] != \"non-hostile\" and row[2] != \"fake\" and row[2] != \"defamation\"]\n",
    "\n",
    "false_neg_no_hate = [row for row in no_hate_rows if row[2] == \"non-hostile\" and row[5] != \"No Hate\"]\n",
    "false_neg_weak_hate = [row for row in weak_hate_rows if row[2] == \"fake\" or row[2] == \"defamation\" and row[5] != \"weakly hateful\"]\n",
    "false_neg_strong_hate = [row for row in strong_hate_rows if row[2] != \"non-hostile\" and row[2] != \"fake\" and row[2] != \"defamation\" and row[5] != \"strongly hateful\"]\n",
    "\n",
    "precision = (len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows))/(len(no_hate_rows)+len(strong_hate_rows)+len(weak_hate_rows))\n",
    "recall = (len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows))/(len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows)+len(false_neg_no_hate)+len(false_neg_strong_hate)+len(false_neg_weak_hate))\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print(\"Total no. of rows: {}\".format(len(total_rows)))\n",
    "print(\"No Hate: {}\".format(len(no_hate_rows)))\n",
    "print(\"Actual no hate: {}\".format(len(correct_no_hate_rows)))\n",
    "print(\"Weak Hate: {}\".format(len(weak_hate_rows)))\n",
    "print(\"Actual weak hate: {}\".format(len(correct_weak_hate_rows)))\n",
    "print(\"Strong Hate: {}\".format(len(strong_hate_rows)))\n",
    "print(\"Actual strong hate: {}\".format(len(correct_strong_hate_rows)))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "print(\"F-score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBg1o7MSZeox"
   },
   "source": [
    "### Semantic + Hate Lexicon + Thematic Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5WzWpPKWrN21",
    "outputId": "0a2c4bc0-18e7-4f87-f814-4b49443a5a1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of rows: 811\n",
      "No Hate: 473\n",
      "Actual no hate: 280\n",
      "Weak Hate: 208\n",
      "Actual weak hate: 51\n",
      "Strong Hate: 130\n",
      "Actual strong hate: 63\n",
      "Precision: 0.48581997533908755\n",
      "Recall: 0.9184149184149184\n",
      "F-score: 0.635483870967742\n"
     ]
    }
   ],
   "source": [
    "for row in rows:           \n",
    "  if row[3] <= -0.5 or row[3] >= 1: \n",
    "    strongcount = 0\n",
    "    hlexcount = 0 \n",
    "    weakcount = 0                   \n",
    "    themecount = 0\n",
    "    if any([word in row[1] for word in strongly_negative_words]):\n",
    "      strongcount += 1 \n",
    "    if any([word in row[1] for word in hlex]):\n",
    "      hlexcount += 1\n",
    "    if any([word in row[1] for word in weakly_negative_words]):\n",
    "      weakcount += 1                 \n",
    "    if any([word in row[1] for word in themenouns]):\n",
    "      themecount += 1\n",
    "\n",
    "    if strongcount >= 2: \n",
    "        row[5] = \"strongly hateful\"  \n",
    "    elif strongcount == 1:\n",
    "      if hlexcount >= 1 or themecount >= 1:\n",
    "        row[5] = \"strongly hateful\" \n",
    "      else: \n",
    "        row[5] = \"weakly hateful\"              \n",
    "    elif strongcount == 0:     \n",
    "      if themecount >= 1 and hlexcount >= 1:  \n",
    "        row[5] = \"strongly hateful\"   \n",
    "      elif themecount >=1 and weakcount >= 1: \n",
    "        row[5] = \"weakly hateful\" \n",
    "      elif hlexcount == 1:  \n",
    "        row[5] = \"weakly hateful\" \n",
    "      else:         \n",
    "        row[5] = \"No Hate\" \n",
    "\n",
    "\n",
    "total_rows = [row for row in rows]\n",
    "\n",
    "no_hate_rows = [row for row in rows if row[5] == \"No Hate\"]\n",
    "correct_no_hate_rows = [row for row in no_hate_rows if row[5] == \"No Hate\" and row[2] == \"non-hostile\"]\n",
    "weak_hate_rows = [row for row in rows if row[5] == \"weakly hateful\"]\n",
    "correct_weak_hate_rows = [row for row in weak_hate_rows if row[5] == \"weakly hateful\" and (row[2] == \"fake\" or row[2] == \"defamation\")]\n",
    "strong_hate_rows = [row for row in rows if row[5] == \"strongly hateful\"]\n",
    "correct_strong_hate_rows = [row for row in strong_hate_rows if row[5] == \"strongly hateful\" and row[2] != \"non-hostile\" and row[2] != \"fake\" and row[2] != \"defamation\"]\n",
    "\n",
    "false_neg_no_hate = [row for row in no_hate_rows if row[2] == \"non-hostile\" and row[5] != \"No Hate\"]\n",
    "false_neg_weak_hate = [row for row in weak_hate_rows if row[2] == \"fake\" or row[2] == \"defamation\" and row[5] != \"weakly hateful\"]\n",
    "false_neg_strong_hate = [row for row in strong_hate_rows if row[2] != \"non-hostile\" and row[2] != \"fake\" and row[2] != \"defamation\" and row[5] != \"strongly hateful\"]\n",
    "\n",
    "precision = (len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows))/(len(no_hate_rows)+len(strong_hate_rows)+len(weak_hate_rows))\n",
    "recall = (len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows))/(len(correct_no_hate_rows)+len(correct_strong_hate_rows)+len(correct_weak_hate_rows)+len(false_neg_no_hate)+len(false_neg_strong_hate)+len(false_neg_weak_hate))\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print(\"Total no. of rows: {}\".format(len(total_rows)))\n",
    "print(\"No Hate: {}\".format(len(no_hate_rows)))\n",
    "print(\"Actual no hate: {}\".format(len(correct_no_hate_rows)))\n",
    "print(\"Weak Hate: {}\".format(len(weak_hate_rows)))\n",
    "print(\"Actual weak hate: {}\".format(len(correct_weak_hate_rows)))\n",
    "print(\"Strong Hate: {}\".format(len(strong_hate_rows)))\n",
    "print(\"Actual strong hate: {}\".format(len(correct_strong_hate_rows)))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "print(\"F-score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzwZW-sZYxQF"
   },
   "source": [
    "## Exporting results into results.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "SVeHBwzkOL5N"
   },
   "outputs": [],
   "source": [
    "import csv                                                \n",
    "\n",
    "fields = ['Unique ID', 'Post', 'Labels Set', 'Total Score', 'Hate Label' ,'Subjective Hate Label'] \n",
    "with open(\"results.csv\", 'w', encoding=\"utf8\") as csvfile:                                                       \n",
    " \n",
    "    csvwriter = csv.writer(csvfile) \n",
    "        \n",
    "  \n",
    "    csvwriter.writerow(fields) \n",
    "        \n",
    "    csvwriter.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Hate Speech Detection",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b9f83fe498f40f295d289003ab5ede2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "550df98871f34af690e0b2ed26ec82a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f05541c100ed484690cd8f1b58ad11a7",
      "max": 23856,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cfc25ba9617647b4b9d5c75b066d54e6",
      "value": 23856
     }
    },
    "726428bac2554499bea783078bb6593e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_550df98871f34af690e0b2ed26ec82a1",
       "IPY_MODEL_d7e477fe951543b590f2cf6eafaab77a"
      ],
      "layout": "IPY_MODEL_0b9f83fe498f40f295d289003ab5ede2"
     }
    },
    "b229c205c4de44a1885f836170339816": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cfc25ba9617647b4b9d5c75b066d54e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d7e477fe951543b590f2cf6eafaab77a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3873f326dd646618515defee0898d93",
      "placeholder": "​",
      "style": "IPY_MODEL_b229c205c4de44a1885f836170339816",
      "value": " 139k/? [00:00&lt;00:00, 3.05MB/s]"
     }
    },
    "f05541c100ed484690cd8f1b58ad11a7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3873f326dd646618515defee0898d93": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
